2025-03-26 12:51:09 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: yolov8_)
2025-03-26 12:51:09 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.11.7, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Windows-11-10.0.26100-SP0
2025-03-26 12:51:09 [scrapy.addons] INFO: Enabled addons:
[]
2025-03-26 12:51:09 [asyncio] DEBUG: Using selector: SelectSelector
2025-03-26 12:51:09 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2025-03-26 12:51:09 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
2025-03-26 12:51:09 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2025-03-26 12:51:09 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
2025-03-26 12:51:09 [scrapy.extensions.telnet] INFO: Telnet Password: aed065b54d5a30a7
2025-03-26 12:51:09 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2025-03-26 12:51:09 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'yolov8_',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'tupian_.log',
 'NEWSPIDER_MODULE': 'yolov8_.spiders',
 'SPIDER_MODULES': ['yolov8_.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2025-03-26 12:51:09 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-03-26 12:51:09 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-03-26 12:51:09 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-03-26 12:51:09 [scrapy.core.engine] INFO: Spider opened
2025-03-26 12:51:09 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 12:51:09 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-03-26 12:51:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://aspx.sc.chinaz.com/query.aspx?keyword=%E9%94%AE%E7%9B%98%E5%9B%BE%E7%89%87&issale=&classID=11&navindex=0&page=1> (referer: None)
2025-03-26 12:51:10 [scrapy.core.engine] INFO: Closing spider (finished)
2025-03-26 12:51:10 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 321,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 6065,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'elapsed_time_seconds': 0.692339,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 3, 26, 4, 51, 10, 545700, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 47128,
 'httpcompression/response_count': 1,
 'items_per_minute': None,
 'log_count/DEBUG': 6,
 'log_count/INFO': 10,
 'response_received_count': 1,
 'responses_per_minute': None,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2025, 3, 26, 4, 51, 9, 853361, tzinfo=datetime.timezone.utc)}
2025-03-26 12:51:10 [scrapy.core.engine] INFO: Spider closed (finished)
2025-03-26 12:54:27 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: yolov8_)
2025-03-26 12:54:27 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.11.7, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Windows-11-10.0.26100-SP0
2025-03-26 12:54:27 [scrapy.addons] INFO: Enabled addons:
[]
2025-03-26 12:54:27 [asyncio] DEBUG: Using selector: SelectSelector
2025-03-26 12:54:27 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2025-03-26 12:54:27 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
2025-03-26 12:54:27 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2025-03-26 12:54:27 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
2025-03-26 12:54:27 [scrapy.extensions.telnet] INFO: Telnet Password: ff253aa2ee775a67
2025-03-26 12:54:27 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2025-03-26 12:54:27 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'yolov8_',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'tupian_.log',
 'NEWSPIDER_MODULE': 'yolov8_.spiders',
 'SPIDER_MODULES': ['yolov8_.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2025-03-26 12:54:28 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-03-26 12:54:28 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-03-26 12:54:28 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-03-26 12:54:28 [scrapy.core.engine] INFO: Spider opened
2025-03-26 12:54:28 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 12:54:28 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-03-26 12:54:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian.html> (referer: None)
2025-03-26 12:54:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_2.html> (referer: https://sc.chinaz.com/tupian/huacaotupian.html)
2025-03-26 12:54:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_4.html> (referer: https://sc.chinaz.com/tupian/huacaotupian.html)
2025-03-26 12:54:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_6.html> (referer: https://sc.chinaz.com/tupian/huacaotupian.html)
2025-03-26 12:54:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_3.html> (referer: https://sc.chinaz.com/tupian/huacaotupian.html)
2025-03-26 12:54:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_5.html> (referer: https://sc.chinaz.com/tupian/huacaotupian.html)
2025-03-26 12:54:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_331.html> (referer: https://sc.chinaz.com/tupian/huacaotupian.html)
2025-03-26 12:54:42 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://sc.chinaz.com/tupian/huacaotupian_3.html> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2025-03-26 12:55:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_331.html> (referer: https://sc.chinaz.com/tupian/huacaotupian.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-26 12:55:50 [scrapy.extensions.logstats] INFO: Crawled 7 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 12:55:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_8.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_4.html)
2025-03-26 12:55:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_7.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_2.html)
2025-03-26 12:55:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_10.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_6.html)
2025-03-26 12:55:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_9.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_4.html)
2025-03-26 12:55:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_11.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_6.html)
2025-03-26 12:57:09 [scrapy.extensions.logstats] INFO: Crawled 12 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 12:57:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_13.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_11.html)
2025-03-26 12:57:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_15.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_11.html)
2025-03-26 12:57:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_12.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_11.html)
2025-03-26 12:57:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_14.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_11.html)
2025-03-26 12:57:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_16.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_11.html)
2025-03-26 12:58:22 [scrapy.extensions.logstats] INFO: Crawled 17 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 12:58:41 [scrapy.extensions.logstats] INFO: Crawled 17 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 12:58:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_18.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_15.html)
2025-03-26 12:58:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_17.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_15.html)
2025-03-26 12:58:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_20.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_15.html)
2025-03-26 12:58:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_21.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_16.html)
2025-03-26 12:58:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_19.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_15.html)
2025-03-26 13:00:02 [scrapy.extensions.logstats] INFO: Crawled 22 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:00:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_22.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_21.html)
2025-03-26 13:00:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_25.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_21.html)
2025-03-26 13:00:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_23.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_21.html)
2025-03-26 13:00:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_26.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_21.html)
2025-03-26 13:00:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_24.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_21.html)
2025-03-26 13:00:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_293.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_21.html)
2025-03-26 13:00:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_24.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_21.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:01:12 [scrapy.extensions.logstats] INFO: Crawled 28 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:01:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_292.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_293.html)
2025-03-26 13:01:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_290.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_293.html)
2025-03-26 13:01:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_288.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_293.html)
2025-03-26 13:01:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_291.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_293.html)
2025-03-26 13:01:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_294.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_293.html)
2025-03-26 13:01:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_27.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_26.html)
2025-03-26 13:01:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_289.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_293.html)
2025-03-26 13:01:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_295.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_293.html)
2025-03-26 13:01:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_296.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_293.html)
2025-03-26 13:01:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_29.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_26.html)
2025-03-26 13:01:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_297.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_293.html)
2025-03-26 13:01:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_28.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_26.html)
2025-03-26 13:01:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_31.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_26.html)
2025-03-26 13:01:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_30.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_26.html)
2025-03-26 13:01:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_298.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_293.html)
2025-03-26 13:01:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_313.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_293.html)
2025-03-26 13:03:30 [scrapy.extensions.logstats] INFO: Crawled 44 pages (at 16 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:03:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_28.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_26.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:04:29 [scrapy.extensions.logstats] INFO: Crawled 44 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:04:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_287.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_292.html)
2025-03-26 13:04:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_286.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_291.html)
2025-03-26 13:04:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_285.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_290.html)
2025-03-26 13:04:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_284.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_289.html)
2025-03-26 13:04:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_283.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_288.html)
2025-03-26 13:04:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_308.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_313.html)
2025-03-26 13:04:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_312.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_313.html)
2025-03-26 13:04:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_309.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_313.html)
2025-03-26 13:04:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_315.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_313.html)
2025-03-26 13:04:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_301.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_297.html)
2025-03-26 13:04:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_32.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_29.html)
2025-03-26 13:04:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_310.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_313.html)
2025-03-26 13:04:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_299.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_297.html)
2025-03-26 13:04:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_300.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_297.html)
2025-03-26 13:04:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_311.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_313.html)
2025-03-26 13:04:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_314.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_313.html)
2025-03-26 13:04:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_286.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_291.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:05:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_312.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_313.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:05:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_318.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_313.html)
2025-03-26 13:05:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_303.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_298.html)
2025-03-26 13:05:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_36.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_31.html)
2025-03-26 13:05:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_35.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_31.html)
2025-03-26 13:05:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_317.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_297.html)
2025-03-26 13:05:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_316.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_313.html)
2025-03-26 13:05:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_302.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_297.html)
2025-03-26 13:05:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_34.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_29.html)
2025-03-26 13:05:51 [scrapy.extensions.logstats] INFO: Crawled 68 pages (at 24 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:06:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_310.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_313.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:06:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_299.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_297.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:06:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_300.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_297.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:06:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_311.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_313.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:06:43 [scrapy.extensions.logstats] INFO: Crawled 68 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:06:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_303.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_298.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:06:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_36.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_31.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:06:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_35.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_31.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:07:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_317.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_297.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:07:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_33.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_29.html)
2025-03-26 13:07:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_307.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_308.html)
2025-03-26 13:07:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_282.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_287.html)
2025-03-26 13:07:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_278.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_283.html)
2025-03-26 13:07:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_280.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_285.html)
2025-03-26 13:07:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_304.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_309.html)
2025-03-26 13:07:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_279.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_284.html)
2025-03-26 13:07:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_305.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_309.html)
2025-03-26 13:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_281.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_285.html)
2025-03-26 13:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_319.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_318.html)
2025-03-26 13:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_306.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_309.html)
2025-03-26 13:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_321.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_318.html)
2025-03-26 13:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_328.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_308.html)
2025-03-26 13:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_320.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_318.html)
2025-03-26 13:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_329.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_309.html)
2025-03-26 13:07:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_37.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_34.html)
2025-03-26 13:07:45 [scrapy.extensions.logstats] INFO: Crawled 84 pages (at 16 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:07:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_278.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_283.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:07:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_279.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_284.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:08:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_305.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_309.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:08:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_39.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_34.html)
2025-03-26 13:08:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_38.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_34.html)
2025-03-26 13:08:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_322.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_318.html)
2025-03-26 13:08:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_323.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_318.html)
2025-03-26 13:08:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_306.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_309.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:08:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_321.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_318.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:08:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_328.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_308.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-26 13:08:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_329.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_309.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:08:58 [scrapy.extensions.logstats] INFO: Crawled 88 pages (at 4 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:09:46 [scrapy.extensions.logstats] INFO: Crawled 88 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:09:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_277.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_282.html)
2025-03-26 13:09:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_275.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_280.html)
2025-03-26 13:09:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_327.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_307.html)
2025-03-26 13:09:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_324.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_323.html)
2025-03-26 13:09:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_276.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_280.html)
2025-03-26 13:09:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_41.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_39.html)
2025-03-26 13:09:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_325.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_323.html)
2025-03-26 13:09:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_40.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_39.html)
2025-03-26 13:09:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_326.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_323.html)
2025-03-26 13:09:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_44.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_39.html)
2025-03-26 13:09:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_43.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_39.html)
2025-03-26 13:09:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_42.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_39.html)
2025-03-26 13:09:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_275.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_280.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:10:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_327.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_307.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-26 13:11:38 [scrapy.extensions.logstats] INFO: Crawled 100 pages (at 12 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:11:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_44.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_39.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:12:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_272.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_277.html)
2025-03-26 13:12:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_273.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_277.html)
2025-03-26 13:12:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_46.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_41.html)
2025-03-26 13:12:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_45.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_41.html)
2025-03-26 13:12:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_274.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_277.html)
2025-03-26 13:12:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_271.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_276.html)
2025-03-26 13:12:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_330.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_326.html)
2025-03-26 13:12:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_47.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_43.html)
2025-03-26 13:12:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_48.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_43.html)
2025-03-26 13:12:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_273.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_277.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:12:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_46.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_41.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:12:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_45.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_41.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:12:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_330.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_326.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-26 13:12:46 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_47.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_43.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:13:01 [scrapy.extensions.logstats] INFO: Crawled 109 pages (at 9 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:13:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_270.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_271.html)
2025-03-26 13:13:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_49.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_48.html)
2025-03-26 13:13:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_269.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_272.html)
2025-03-26 13:13:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_267.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_272.html)
2025-03-26 13:13:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_266.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_271.html)
2025-03-26 13:13:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_268.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_272.html)
2025-03-26 13:13:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_50.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_48.html)
2025-03-26 13:13:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_51.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_48.html)
2025-03-26 13:13:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_53.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_48.html)
2025-03-26 13:13:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_49.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_48.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:13:42 [scrapy.extensions.logstats] INFO: Crawled 118 pages (at 9 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:13:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_52.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_48.html)
2025-03-26 13:14:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_51.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_48.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:14:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_53.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_48.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:14:35 [scrapy.extensions.logstats] INFO: Crawled 119 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:14:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_265.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_270.html)
2025-03-26 13:14:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_262.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_267.html)
2025-03-26 13:14:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_264.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_269.html)
2025-03-26 13:14:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_263.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_267.html)
2025-03-26 13:14:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_261.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_266.html)
2025-03-26 13:14:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_55.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_52.html)
2025-03-26 13:14:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_54.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_52.html)
2025-03-26 13:14:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_56.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_52.html)
2025-03-26 13:14:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_57.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_52.html)
2025-03-26 13:14:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_265.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_270.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:14:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_264.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_269.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:15:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_55.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_52.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:15:44 [scrapy.extensions.logstats] INFO: Crawled 128 pages (at 9 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:15:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_258.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_262.html)
2025-03-26 13:15:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_257.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_262.html)
2025-03-26 13:15:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_256.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_261.html)
2025-03-26 13:15:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_259.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_262.html)
2025-03-26 13:15:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_59.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_57.html)
2025-03-26 13:15:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_260.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_261.html)
2025-03-26 13:15:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_60.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_57.html)
2025-03-26 13:15:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_61.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_57.html)
2025-03-26 13:15:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_62.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_57.html)
2025-03-26 13:16:35 [scrapy.extensions.logstats] INFO: Crawled 137 pages (at 9 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:16:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_58.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_57.html)
2025-03-26 13:16:54 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_59.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_57.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:17:56 [scrapy.extensions.logstats] INFO: Crawled 138 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:17:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_58.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_57.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:17:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_254.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_258.html)
2025-03-26 13:17:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_255.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_256.html)
2025-03-26 13:17:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_251.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_256.html)
2025-03-26 13:17:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_63.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_62.html)
2025-03-26 13:17:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_253.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_258.html)
2025-03-26 13:17:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_252.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_257.html)
2025-03-26 13:17:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_65.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_62.html)
2025-03-26 13:17:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_64.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_62.html)
2025-03-26 13:17:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_66.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_62.html)
2025-03-26 13:17:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_67.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_62.html)
2025-03-26 13:18:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_255.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_256.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:18:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_251.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_256.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:18:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_252.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_257.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:19:00 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_66.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_62.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:19:10 [scrapy.extensions.logstats] INFO: Crawled 148 pages (at 10 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:19:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_248.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_253.html)
2025-03-26 13:19:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_249.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_254.html)
2025-03-26 13:19:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_250.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_254.html)
2025-03-26 13:19:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_69.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_67.html)
2025-03-26 13:19:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_72.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_67.html)
2025-03-26 13:19:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_70.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_67.html)
2025-03-26 13:19:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_71.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_67.html)
2025-03-26 13:19:11 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_68.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_67.html)
2025-03-26 13:19:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_249.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_254.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:19:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_72.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_67.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:20:25 [scrapy.extensions.logstats] INFO: Crawled 156 pages (at 8 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:20:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_243.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_248.html)
2025-03-26 13:20:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_244.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_248.html)
2025-03-26 13:20:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_245.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_250.html)
2025-03-26 13:20:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_75.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_71.html)
2025-03-26 13:20:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_73.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_71.html)
2025-03-26 13:20:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_247.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_248.html)
2025-03-26 13:20:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_246.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_250.html)
2025-03-26 13:20:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_74.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_71.html)
2025-03-26 13:20:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_76.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_71.html)
2025-03-26 13:20:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_243.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_248.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:20:40 [scrapy.extensions.logstats] INFO: Crawled 165 pages (at 9 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:21:54 [scrapy.extensions.logstats] INFO: Crawled 165 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:21:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_241.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_246.html)
2025-03-26 13:21:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_240.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_244.html)
2025-03-26 13:21:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_242.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_247.html)
2025-03-26 13:21:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_77.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_76.html)
2025-03-26 13:21:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_80.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_76.html)
2025-03-26 13:21:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_78.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_76.html)
2025-03-26 13:21:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_239.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_244.html)
2025-03-26 13:21:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_79.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_76.html)
2025-03-26 13:21:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_81.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_76.html)
2025-03-26 13:22:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_80.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_76.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:23:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_239.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_244.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:23:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_81.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_76.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:23:25 [scrapy.extensions.logstats] INFO: Crawled 174 pages (at 9 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:23:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_238.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_241.html)
2025-03-26 13:23:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_235.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_240.html)
2025-03-26 13:23:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_237.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_241.html)
2025-03-26 13:23:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_236.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_241.html)
2025-03-26 13:23:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_83.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_79.html)
2025-03-26 13:23:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_82.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_79.html)
2025-03-26 13:23:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_84.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_79.html)
2025-03-26 13:23:57 [scrapy.extensions.logstats] INFO: Crawled 181 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:24:09 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_83.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_79.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:24:39 [scrapy.extensions.logstats] INFO: Crawled 181 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:24:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_85.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_84.html)
2025-03-26 13:24:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_233.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_238.html)
2025-03-26 13:24:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_231.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_235.html)
2025-03-26 13:24:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_234.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_235.html)
2025-03-26 13:24:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_86.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_84.html)
2025-03-26 13:24:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_232.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_237.html)
2025-03-26 13:24:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_230.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_235.html)
2025-03-26 13:24:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_87.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_84.html)
2025-03-26 13:24:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_85.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_84.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:24:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_231.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_235.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:25:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_232.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_237.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:25:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_88.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_84.html)
2025-03-26 13:25:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_89.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_84.html)
2025-03-26 13:25:38 [scrapy.extensions.logstats] INFO: Crawled 191 pages (at 10 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:25:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_88.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_84.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:25:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_228.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_233.html)
2025-03-26 13:25:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_225.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_230.html)
2025-03-26 13:25:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_229.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_230.html)
2025-03-26 13:26:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_229.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_230.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:26:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_93.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_89.html)
2025-03-26 13:26:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_91.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_86.html)
2025-03-26 13:26:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_90.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_86.html)
2025-03-26 13:26:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_92.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_87.html)
2025-03-26 13:26:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_94.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_89.html)
2025-03-26 13:26:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_227.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_230.html)
2025-03-26 13:26:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_226.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_230.html)
2025-03-26 13:26:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_93.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_89.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:26:28 [scrapy.extensions.logstats] INFO: Crawled 201 pages (at 10 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:26:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_224.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_225.html)
2025-03-26 13:26:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_220.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_225.html)
2025-03-26 13:26:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_223.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_228.html)
2025-03-26 13:26:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_221.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_225.html)
2025-03-26 13:26:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_222.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_225.html)
2025-03-26 13:27:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_227.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_230.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:27:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_226.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_230.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:28:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_221.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_225.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:28:27 [scrapy.extensions.logstats] INFO: Crawled 206 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:28:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_217.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_222.html)
2025-03-26 13:28:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_215.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_220.html)
2025-03-26 13:28:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_95.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_94.html)
2025-03-26 13:28:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_216.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_220.html)
2025-03-26 13:28:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_218.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_223.html)
2025-03-26 13:28:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_219.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_220.html)
2025-03-26 13:28:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_96.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_94.html)
2025-03-26 13:28:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_97.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_94.html)
2025-03-26 13:28:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_99.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_94.html)
2025-03-26 13:28:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_98.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_94.html)
2025-03-26 13:28:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_217.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_222.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:28:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_215.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_220.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:28:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_95.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_94.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:28:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_219.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_220.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:29:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_98.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_94.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:29:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_96.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_94.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:29:18 [scrapy.extensions.logstats] INFO: Crawled 216 pages (at 10 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:29:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_213.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_218.html)
2025-03-26 13:29:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_102.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_99.html)
2025-03-26 13:29:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_211.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_216.html)
2025-03-26 13:29:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_100.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_99.html)
2025-03-26 13:29:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_101.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_99.html)
2025-03-26 13:29:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_212.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_216.html)
2025-03-26 13:29:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_214.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_218.html)
2025-03-26 13:29:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_103.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_99.html)
2025-03-26 13:29:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_104.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_99.html)
2025-03-26 13:29:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_213.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_218.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:29:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_211.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_216.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:29:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_100.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_99.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:29:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_214.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_218.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:30:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_104.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_99.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:30:02 [scrapy.extensions.logstats] INFO: Crawled 225 pages (at 9 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:30:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_209.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_212.html)
2025-03-26 13:30:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_207.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_212.html)
2025-03-26 13:30:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_210.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_212.html)
2025-03-26 13:30:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_208.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_212.html)
2025-03-26 13:30:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_106.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_103.html)
2025-03-26 13:30:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_105.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_103.html)
2025-03-26 13:30:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_107.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_103.html)
2025-03-26 13:30:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_108.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_103.html)
2025-03-26 13:30:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_210.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_212.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:30:35 [scrapy.extensions.logstats] INFO: Crawled 233 pages (at 8 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:30:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_106.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_103.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:30:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_105.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_103.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:30:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_203.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_208.html)
2025-03-26 13:30:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_202.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_207.html)
2025-03-26 13:30:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_206.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_207.html)
2025-03-26 13:30:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_204.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_209.html)
2025-03-26 13:30:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_205.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_209.html)
2025-03-26 13:30:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_110.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_108.html)
2025-03-26 13:30:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_109.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_108.html)
2025-03-26 13:30:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_111.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_108.html)
2025-03-26 13:30:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_113.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_108.html)
2025-03-26 13:30:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_112.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_108.html)
2025-03-26 13:30:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_203.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_208.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:31:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_204.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_209.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:31:37 [scrapy.extensions.logstats] INFO: Crawled 243 pages (at 10 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:31:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_110.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_108.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:31:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_111.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_108.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:32:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_112.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_108.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:32:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_201.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_202.html)
2025-03-26 13:32:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_197.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_202.html)
2025-03-26 13:32:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_200.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_205.html)
2025-03-26 13:32:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_199.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_202.html)
2025-03-26 13:32:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_198.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_202.html)
2025-03-26 13:32:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_115.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_113.html)
2025-03-26 13:32:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_116.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_113.html)
2025-03-26 13:32:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_114.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_113.html)
2025-03-26 13:32:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_117.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_113.html)
2025-03-26 13:32:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_118.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_113.html)
2025-03-26 13:32:33 [scrapy.extensions.logstats] INFO: Crawled 253 pages (at 10 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:32:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_199.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_202.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:33:01 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_116.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_113.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:33:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_118.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_113.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:33:29 [scrapy.extensions.logstats] INFO: Crawled 253 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:33:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_196.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_197.html)
2025-03-26 13:33:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_193.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_197.html)
2025-03-26 13:33:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_192.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_197.html)
2025-03-26 13:33:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_120.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_117.html)
2025-03-26 13:33:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_195.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_200.html)
2025-03-26 13:33:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_121.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_117.html)
2025-03-26 13:33:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_194.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_197.html)
2025-03-26 13:33:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_119.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_117.html)
2025-03-26 13:33:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_122.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_117.html)
2025-03-26 13:33:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_196.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_197.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:33:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_193.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_197.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:33:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_192.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_197.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:33:29 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_195.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_200.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:33:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_120.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_117.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:33:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_194.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_197.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:34:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_127.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_122.html)
2025-03-26 13:34:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_125.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_122.html)
2025-03-26 13:34:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_124.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_122.html)
2025-03-26 13:34:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_126.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_122.html)
2025-03-26 13:34:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_123.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_122.html)
2025-03-26 13:34:12 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_125.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_122.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:34:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_126.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_122.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:34:37 [scrapy.extensions.logstats] INFO: Crawled 267 pages (at 14 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:34:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_130.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_127.html)
2025-03-26 13:34:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_129.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_127.html)
2025-03-26 13:34:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_128.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_127.html)
2025-03-26 13:34:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_132.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_127.html)
2025-03-26 13:34:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_131.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_127.html)
2025-03-26 13:35:30 [scrapy.extensions.logstats] INFO: Crawled 272 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:35:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_133.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_132.html)
2025-03-26 13:35:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_134.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_132.html)
2025-03-26 13:35:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_136.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_132.html)
2025-03-26 13:35:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_137.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_132.html)
2025-03-26 13:35:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_135.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_132.html)
2025-03-26 13:35:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_134.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_132.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:36:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_138.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_136.html)
2025-03-26 13:36:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_142.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_137.html)
2025-03-26 13:36:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_139.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_136.html)
2025-03-26 13:36:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_141.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_136.html)
2025-03-26 13:36:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_140.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_136.html)
2025-03-26 13:36:31 [scrapy.extensions.logstats] INFO: Crawled 282 pages (at 10 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:37:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_144.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_142.html)
2025-03-26 13:37:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_146.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_142.html)
2025-03-26 13:37:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_143.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_142.html)
2025-03-26 13:37:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_147.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_142.html)
2025-03-26 13:37:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_145.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_142.html)
2025-03-26 13:37:18 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_144.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_142.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:37:33 [scrapy.extensions.logstats] INFO: Crawled 287 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:37:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_143.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_142.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:37:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_147.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_142.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:37:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_150.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_146.html)
2025-03-26 13:37:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_149.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_146.html)
2025-03-26 13:37:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_151.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_146.html)
2025-03-26 13:37:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_148.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_146.html)
2025-03-26 13:37:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_150.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_146.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:37:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_149.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_146.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:38:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_148.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_146.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:38:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_155.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_151.html)
2025-03-26 13:38:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_152.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_151.html)
2025-03-26 13:38:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_153.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_151.html)
2025-03-26 13:38:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_156.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_151.html)
2025-03-26 13:38:02 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_154.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_151.html)
2025-03-26 13:38:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_155.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_151.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:38:50 [scrapy.extensions.logstats] INFO: Crawled 296 pages (at 9 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:38:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_157.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_156.html)
2025-03-26 13:38:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_158.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_156.html)
2025-03-26 13:38:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_159.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_156.html)
2025-03-26 13:38:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_160.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_156.html)
2025-03-26 13:38:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_161.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_156.html)
2025-03-26 13:39:11 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_159.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_156.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:39:37 [scrapy.extensions.logstats] INFO: Crawled 301 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:39:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_162.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_161.html)
2025-03-26 13:39:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_164.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_161.html)
2025-03-26 13:39:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_166.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_161.html)
2025-03-26 13:39:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_163.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_161.html)
2025-03-26 13:39:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_165.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_161.html)
2025-03-26 13:40:32 [scrapy.extensions.logstats] INFO: Crawled 306 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:40:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_167.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_166.html)
2025-03-26 13:40:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_170.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_166.html)
2025-03-26 13:40:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_169.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_166.html)
2025-03-26 13:40:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_168.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_166.html)
2025-03-26 13:40:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_171.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_166.html)
2025-03-26 13:41:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_171.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_166.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:41:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/huacaotupian_168.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_166.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./huacao/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 13:41:31 [scrapy.extensions.logstats] INFO: Crawled 311 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:41:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_174.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_170.html)
2025-03-26 13:41:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_175.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_170.html)
2025-03-26 13:41:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_172.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_170.html)
2025-03-26 13:41:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_173.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_170.html)
2025-03-26 13:42:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_178.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_175.html)
2025-03-26 13:42:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_177.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_175.html)
2025-03-26 13:42:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_179.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_175.html)
2025-03-26 13:42:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_176.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_175.html)
2025-03-26 13:42:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_180.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_175.html)
2025-03-26 13:43:12 [scrapy.extensions.logstats] INFO: Crawled 320 pages (at 9 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:43:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_182.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_179.html)
2025-03-26 13:43:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_181.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_179.html)
2025-03-26 13:43:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_183.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_179.html)
2025-03-26 13:43:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_184.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_179.html)
2025-03-26 13:43:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_185.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_180.html)
2025-03-26 13:43:50 [scrapy.extensions.logstats] INFO: Crawled 325 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:44:28 [scrapy.extensions.logstats] INFO: Crawled 325 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:44:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_188.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_185.html)
2025-03-26 13:44:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_187.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_185.html)
2025-03-26 13:44:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_190.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_185.html)
2025-03-26 13:44:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_189.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_185.html)
2025-03-26 13:44:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_186.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_185.html)
2025-03-26 13:45:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/huacaotupian_191.html> (referer: https://sc.chinaz.com/tupian/huacaotupian_190.html)
2025-03-26 13:45:39 [scrapy.extensions.logstats] INFO: Crawled 331 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 13:45:39 [scrapy.core.engine] INFO: Closing spider (finished)
2025-03-26 13:45:39 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 103304,
 'downloader/request_count': 331,
 'downloader/request_method_count/GET': 331,
 'downloader/response_bytes': 2111586,
 'downloader/response_count': 331,
 'downloader/response_status_count/200': 331,
 'dupefilter/filtered': 2223,
 'elapsed_time_seconds': 3071.548393,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 3, 26, 5, 45, 39, 909807, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 11855302,
 'httpcompression/response_count': 331,
 'items_per_minute': None,
 'log_count/DEBUG': 337,
 'log_count/ERROR': 97,
 'log_count/INFO': 58,
 'request_depth_max': 42,
 'response_received_count': 331,
 'responses_per_minute': None,
 'scheduler/dequeued': 331,
 'scheduler/dequeued/memory': 331,
 'scheduler/enqueued': 331,
 'scheduler/enqueued/memory': 331,
 'spider_exceptions/HTTPError': 97,
 'start_time': datetime.datetime(2025, 3, 26, 4, 54, 28, 361414, tzinfo=datetime.timezone.utc)}
2025-03-26 13:45:39 [scrapy.core.engine] INFO: Spider closed (finished)
2025-03-26 14:19:07 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: yolov8_)
2025-03-26 14:19:07 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.11.7, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Windows-11-10.0.26100-SP0
2025-03-26 14:19:07 [scrapy.addons] INFO: Enabled addons:
[]
2025-03-26 14:19:07 [asyncio] DEBUG: Using selector: SelectSelector
2025-03-26 14:19:07 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2025-03-26 14:19:07 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
2025-03-26 14:19:07 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2025-03-26 14:19:07 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
2025-03-26 14:19:07 [scrapy.extensions.telnet] INFO: Telnet Password: 59191d1045474a4f
2025-03-26 14:19:07 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2025-03-26 14:19:07 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'yolov8_',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'tupian_.log',
 'NEWSPIDER_MODULE': 'yolov8_.spiders',
 'SPIDER_MODULES': ['yolov8_.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2025-03-26 14:19:07 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-03-26 14:19:07 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-03-26 14:19:07 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-03-26 14:19:07 [scrapy.core.engine] INFO: Spider opened
2025-03-26 14:19:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 14:19:07 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-03-26 14:19:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian.html> (referer: None)
2025-03-26 14:19:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_2.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian.html)
2025-03-26 14:19:24 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_3.html> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2025-03-26 14:19:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_116.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian.html)
2025-03-26 14:19:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_4.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian.html)
2025-03-26 14:19:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_3.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian.html)
2025-03-26 14:19:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_5.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian.html)
2025-03-26 14:19:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_6.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian.html)
2025-03-26 14:19:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_116.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shenghuoyongpin/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-26 14:20:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_7.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_2.html)
2025-03-26 14:20:22 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 8 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 14:20:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_7.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_2.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shenghuoyongpin/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-26 14:20:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_8.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_4.html)
2025-03-26 14:20:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_11.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_6.html)
2025-03-26 14:20:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_10.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_6.html)
2025-03-26 14:20:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_9.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_4.html)
2025-03-26 14:21:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_12.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_8.html)
2025-03-26 14:21:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_13.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_8.html)
2025-03-26 14:21:30 [scrapy.extensions.logstats] INFO: Crawled 14 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 14:21:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_16.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_11.html)
2025-03-26 14:21:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_15.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_11.html)
2025-03-26 14:21:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_14.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_11.html)
2025-03-26 14:22:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_17.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_13.html)
2025-03-26 14:22:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_18.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_13.html)
2025-03-26 14:22:29 [scrapy.extensions.logstats] INFO: Crawled 19 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 14:22:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_19.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_16.html)
2025-03-26 14:22:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_20.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_16.html)
2025-03-26 14:22:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_21.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_16.html)
2025-03-26 14:22:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_22.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_18.html)
2025-03-26 14:22:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_23.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_18.html)
2025-03-26 14:23:24 [scrapy.extensions.logstats] INFO: Crawled 24 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 14:23:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_23.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_18.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shenghuoyongpin/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-26 14:23:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_24.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_22.html)
2025-03-26 14:23:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_25.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_22.html)
2025-03-26 14:23:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_27.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_22.html)
2025-03-26 14:23:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_26.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_22.html)
2025-03-26 14:23:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_109.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_21.html)
2025-03-26 14:24:38 [scrapy.extensions.logstats] INFO: Crawled 29 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 14:25:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_108.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_109.html)
2025-03-26 14:25:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_104.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_109.html)
2025-03-26 14:25:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_29.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_27.html)
2025-03-26 14:25:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_28.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_27.html)
2025-03-26 14:25:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_106.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_109.html)
2025-03-26 14:25:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_105.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_109.html)
2025-03-26 14:25:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_107.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_109.html)
2025-03-26 14:25:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_110.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_109.html)
2025-03-26 14:25:19 [scrapy.extensions.logstats] INFO: Crawled 37 pages (at 8 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 14:25:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_113.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_109.html)
2025-03-26 14:25:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_32.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_27.html)
2025-03-26 14:25:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_111.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_109.html)
2025-03-26 14:25:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_30.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_27.html)
2025-03-26 14:25:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_31.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_27.html)
2025-03-26 14:25:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_114.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_109.html)
2025-03-26 14:25:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_112.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_109.html)
2025-03-26 14:26:51 [scrapy.extensions.logstats] INFO: Crawled 44 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 14:28:04 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_114.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_109.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shenghuoyongpin/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-26 14:28:20 [scrapy.extensions.logstats] INFO: Crawled 44 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 14:28:20 [scrapy.downloadermiddlewares.retry] DEBUG: Retrying <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_103.html> (failed 1 times): User timeout caused connection failure: Getting https://sc.chinaz.com/tupian/shenghuoyongpintupian_103.html took longer than 180.0 seconds..
2025-03-26 14:28:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_102.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_107.html)
2025-03-26 14:28:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_101.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_106.html)
2025-03-26 14:28:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_103.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_108.html)
2025-03-26 14:28:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_99.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_104.html)
2025-03-26 14:28:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_100.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_105.html)
2025-03-26 14:28:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_33.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_32.html)
2025-03-26 14:28:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_34.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_32.html)
2025-03-26 14:28:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_115.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_113.html)
2025-03-26 14:29:09 [scrapy.extensions.logstats] INFO: Crawled 52 pages (at 8 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 14:29:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_37.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_32.html)
2025-03-26 14:29:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_36.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_32.html)
2025-03-26 14:29:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_35.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_32.html)
2025-03-26 14:29:41 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_115.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_113.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shenghuoyongpin/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-26 14:29:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_37.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_32.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shenghuoyongpin/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 14:29:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_35.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_32.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shenghuoyongpin/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 14:29:50 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_36.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_32.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shenghuoyongpin/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 14:29:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_98.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_99.html)
2025-03-26 14:29:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_95.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_99.html)
2025-03-26 14:29:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_97.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_102.html)
2025-03-26 14:29:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_94.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_99.html)
2025-03-26 14:29:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_96.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_101.html)
2025-03-26 14:30:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_38.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_34.html)
2025-03-26 14:30:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_39.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_34.html)
2025-03-26 14:30:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_94.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_99.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shenghuoyongpin/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 14:30:31 [scrapy.extensions.logstats] INFO: Crawled 62 pages (at 10 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 14:30:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_38.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_34.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shenghuoyongpin/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 14:30:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_39.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_34.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shenghuoyongpin/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 14:30:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_93.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_98.html)
2025-03-26 14:30:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_93.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_98.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shenghuoyongpin/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 14:30:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_90.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_95.html)
2025-03-26 14:30:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_92.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_97.html)
2025-03-26 14:30:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_91.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_96.html)
2025-03-26 14:30:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_86.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_91.html)
2025-03-26 14:30:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_85.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_90.html)
2025-03-26 14:30:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_87.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_92.html)
2025-03-26 14:30:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_88.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_92.html)
2025-03-26 14:30:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_86.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_91.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shenghuoyongpin/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 14:30:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_89.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_90.html)
2025-03-26 14:30:52 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_85.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_90.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shenghuoyongpin/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 14:30:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_87.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_92.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shenghuoyongpin/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 14:30:53 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_88.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_92.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shenghuoyongpin/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 14:30:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_84.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_89.html)
2025-03-26 14:30:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/shenghuoyongpintupian_84.html> (referer: https://sc.chinaz.com/tupian/shenghuoyongpintupian_89.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shenghuoyongpin/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 14:30:59 [scrapy.core.engine] INFO: Closing spider (finished)
2025-03-26 14:30:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 1,
 'downloader/request_bytes': 23949,
 'downloader/request_count': 73,
 'downloader/request_method_count/GET': 73,
 'downloader/response_bytes': 425357,
 'downloader/response_count': 72,
 'downloader/response_status_count/200': 72,
 'dupefilter/filtered': 508,
 'elapsed_time_seconds': 711.723596,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 3, 26, 6, 30, 59, 561988, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 2411174,
 'httpcompression/response_count': 72,
 'items_per_minute': None,
 'log_count/DEBUG': 79,
 'log_count/ERROR': 17,
 'log_count/INFO': 20,
 'request_depth_max': 11,
 'response_received_count': 72,
 'responses_per_minute': None,
 'retry/count': 1,
 'retry/reason_count/twisted.internet.error.TimeoutError': 1,
 'scheduler/dequeued': 73,
 'scheduler/dequeued/memory': 73,
 'scheduler/enqueued': 73,
 'scheduler/enqueued/memory': 73,
 'spider_exceptions/HTTPError': 17,
 'start_time': datetime.datetime(2025, 3, 26, 6, 19, 7, 838392, tzinfo=datetime.timezone.utc)}
2025-03-26 14:30:59 [scrapy.core.engine] INFO: Spider closed (finished)
2025-03-26 14:52:02 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: yolov8_)
2025-03-26 14:52:02 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.11.7, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Windows-11-10.0.26100-SP0
2025-03-26 14:52:02 [scrapy.addons] INFO: Enabled addons:
[]
2025-03-26 14:52:02 [asyncio] DEBUG: Using selector: SelectSelector
2025-03-26 14:52:02 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2025-03-26 14:52:02 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
2025-03-26 14:52:02 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2025-03-26 14:52:02 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
2025-03-26 14:52:02 [scrapy.extensions.telnet] INFO: Telnet Password: a01aec1fd2846701
2025-03-26 14:52:02 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2025-03-26 14:52:02 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'yolov8_',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'tupian_.log',
 'NEWSPIDER_MODULE': 'yolov8_.spiders',
 'SPIDER_MODULES': ['yolov8_.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2025-03-26 14:52:03 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-03-26 14:52:03 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-03-26 14:52:03 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-03-26 14:52:03 [scrapy.core.engine] INFO: Spider opened
2025-03-26 14:52:03 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 14:52:03 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-03-26 14:52:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian.html> (referer: None)
2025-03-26 14:52:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_2.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian.html)
2025-03-26 14:52:24 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://sc.chinaz.com/tupian/zhuangxiutupian_3.html> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2025-03-26 14:52:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_5.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian.html)
2025-03-26 14:52:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_46.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian.html)
2025-03-26 14:52:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_6.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian.html)
2025-03-26 14:52:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_3.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian.html)
2025-03-26 14:52:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_4.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian.html)
2025-03-26 14:52:43 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/zhuangxiutupian_46.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shenghuoyongpin/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-26 14:53:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_7.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_2.html)
2025-03-26 14:53:36 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 8 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 14:53:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_10.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_5.html)
2025-03-26 14:53:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_11.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_6.html)
2025-03-26 14:53:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_9.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_5.html)
2025-03-26 14:53:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_8.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_5.html)
2025-03-26 14:53:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_12.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_7.html)
2025-03-26 14:54:53 [scrapy.extensions.logstats] INFO: Crawled 13 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 14:55:08 [scrapy.extensions.logstats] INFO: Crawled 13 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 14:55:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_15.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_11.html)
2025-03-26 14:55:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_17.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_12.html)
2025-03-26 14:55:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_16.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_11.html)
2025-03-26 14:55:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_14.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_11.html)
2025-03-26 14:56:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_13.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_11.html)
2025-03-26 14:56:14 [scrapy.extensions.logstats] INFO: Crawled 18 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 14:56:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_20.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_17.html)
2025-03-26 14:56:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_18.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_17.html)
2025-03-26 14:56:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_19.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_17.html)
2025-03-26 14:56:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_21.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_17.html)
2025-03-26 14:56:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_22.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_17.html)
2025-03-26 14:57:17 [scrapy.extensions.logstats] INFO: Crawled 23 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 14:57:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_24.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_20.html)
2025-03-26 14:57:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_25.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_20.html)
2025-03-26 14:57:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_26.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_21.html)
2025-03-26 14:57:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_23.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_20.html)
2025-03-26 14:57:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_42.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_22.html)
2025-03-26 14:57:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_41.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_21.html)
2025-03-26 14:57:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_27.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_22.html)
2025-03-26 14:59:08 [scrapy.extensions.logstats] INFO: Crawled 30 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 14:59:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_28.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_27.html)
2025-03-26 14:59:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_38.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_42.html)
2025-03-26 14:59:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_40.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_41.html)
2025-03-26 14:59:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_36.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_41.html)
2025-03-26 14:59:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_29.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_27.html)
2025-03-26 14:59:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_37.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_42.html)
2025-03-26 14:59:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_43.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_42.html)
2025-03-26 14:59:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_39.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_42.html)
2025-03-26 14:59:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_30.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_27.html)
2025-03-26 14:59:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_45.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_42.html)
2025-03-26 14:59:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_44.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_42.html)
2025-03-26 14:59:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_32.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_27.html)
2025-03-26 14:59:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_31.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_27.html)
2025-03-26 15:00:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/zhuangxiutupian_43.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_42.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shenghuoyongpin/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-26 15:01:16 [scrapy.extensions.logstats] INFO: Crawled 43 pages (at 13 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 15:01:16 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/zhuangxiutupian_45.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_42.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shenghuoyongpin/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-26 15:01:17 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/zhuangxiutupian_44.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_42.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shenghuoyongpin/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-26 15:01:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_35.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_36.html)
2025-03-26 15:01:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_33.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_38.html)
2025-03-26 15:01:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_34.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_38.html)
2025-03-26 15:02:05 [scrapy.extensions.logstats] INFO: Crawled 46 pages (at 3 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 15:02:17 [scrapy.core.engine] INFO: Closing spider (finished)
2025-03-26 15:02:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 14484,
 'downloader/request_count': 46,
 'downloader/request_method_count/GET': 46,
 'downloader/response_bytes': 281195,
 'downloader/response_count': 46,
 'downloader/response_status_count/200': 46,
 'dupefilter/filtered': 394,
 'elapsed_time_seconds': 613.808196,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 3, 26, 7, 2, 17, 289358, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 1568570,
 'httpcompression/response_count': 46,
 'items_per_minute': None,
 'log_count/DEBUG': 52,
 'log_count/ERROR': 4,
 'log_count/INFO': 18,
 'request_depth_max': 9,
 'response_received_count': 46,
 'responses_per_minute': None,
 'scheduler/dequeued': 46,
 'scheduler/dequeued/memory': 46,
 'scheduler/enqueued': 46,
 'scheduler/enqueued/memory': 46,
 'spider_exceptions/HTTPError': 4,
 'start_time': datetime.datetime(2025, 3, 26, 6, 52, 3, 481162, tzinfo=datetime.timezone.utc)}
2025-03-26 15:02:17 [scrapy.core.engine] INFO: Spider closed (finished)
2025-03-26 15:14:31 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: yolov8_)
2025-03-26 15:14:31 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.11.7, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Windows-11-10.0.26100-SP0
2025-03-26 15:14:31 [scrapy.addons] INFO: Enabled addons:
[]
2025-03-26 15:14:31 [asyncio] DEBUG: Using selector: SelectSelector
2025-03-26 15:14:31 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2025-03-26 15:14:31 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
2025-03-26 15:14:31 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2025-03-26 15:14:31 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
2025-03-26 15:14:31 [scrapy.extensions.telnet] INFO: Telnet Password: a20e7ccb7cabfeaf
2025-03-26 15:14:31 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2025-03-26 15:14:31 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'yolov8_',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'tupian_.log',
 'NEWSPIDER_MODULE': 'yolov8_.spiders',
 'SPIDER_MODULES': ['yolov8_.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2025-03-26 15:14:32 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-03-26 15:14:32 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-03-26 15:14:32 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-03-26 15:14:32 [scrapy.core.engine] INFO: Spider opened
2025-03-26 15:14:32 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 15:14:32 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-03-26 15:14:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian.html> (referer: None)
2025-03-26 15:14:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_2.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian.html)
2025-03-26 15:14:47 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://sc.chinaz.com/tupian/zhuangxiutupian_3.html> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2025-03-26 15:14:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_46.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian.html)
2025-03-26 15:14:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_6.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian.html)
2025-03-26 15:14:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_4.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian.html)
2025-03-26 15:14:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_3.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian.html)
2025-03-26 15:14:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_5.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian.html)
2025-03-26 15:14:48 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/zhuangxiutupian_46.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shenghuoyongpin/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-26 15:15:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_7.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_2.html)
2025-03-26 15:15:34 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 8 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 15:16:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_9.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_6.html)
2025-03-26 15:16:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_8.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_6.html)
2025-03-26 15:16:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_11.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_6.html)
2025-03-26 15:16:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_12.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_7.html)
2025-03-26 15:16:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_10.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_6.html)
2025-03-26 15:16:55 [scrapy.extensions.logstats] INFO: Crawled 13 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 15:17:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_13.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_9.html)
2025-03-26 15:17:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_14.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_9.html)
2025-03-26 15:17:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_15.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_11.html)
2025-03-26 15:17:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_17.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_12.html)
2025-03-26 15:17:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_16.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_11.html)
2025-03-26 15:18:04 [scrapy.extensions.logstats] INFO: Crawled 18 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 15:18:37 [scrapy.extensions.logstats] INFO: Crawled 18 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 15:18:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_20.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_15.html)
2025-03-26 15:18:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_18.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_15.html)
2025-03-26 15:18:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_19.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_15.html)
2025-03-26 15:18:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_22.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_17.html)
2025-03-26 15:18:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_21.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_17.html)
2025-03-26 15:19:46 [scrapy.extensions.logstats] INFO: Crawled 23 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 15:19:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_23.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_20.html)
2025-03-26 15:19:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_24.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_20.html)
2025-03-26 15:19:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_25.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_20.html)
2025-03-26 15:19:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_27.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_22.html)
2025-03-26 15:19:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_26.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_22.html)
2025-03-26 15:19:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_41.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_21.html)
2025-03-26 15:19:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_42.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_22.html)
2025-03-26 15:21:16 [scrapy.extensions.logstats] INFO: Crawled 30 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 15:21:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_40.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_41.html)
2025-03-26 15:21:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_37.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_42.html)
2025-03-26 15:21:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_39.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_42.html)
2025-03-26 15:21:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_36.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_41.html)
2025-03-26 15:21:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_38.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_42.html)
2025-03-26 15:21:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_28.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_27.html)
2025-03-26 15:21:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_44.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_42.html)
2025-03-26 15:21:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_43.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_42.html)
2025-03-26 15:21:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_30.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_27.html)
2025-03-26 15:21:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_32.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_27.html)
2025-03-26 15:21:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_31.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_27.html)
2025-03-26 15:21:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_29.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_27.html)
2025-03-26 15:21:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_45.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_42.html)
2025-03-26 15:22:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/zhuangxiutupian_44.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_42.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shenghuoyongpin/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-26 15:22:49 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/zhuangxiutupian_43.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_42.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shenghuoyongpin/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-26 15:22:49 [scrapy.extensions.logstats] INFO: Crawled 43 pages (at 13 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 15:23:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/zhuangxiutupian_45.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_42.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shenghuoyongpin/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-26 15:23:44 [scrapy.extensions.logstats] INFO: Crawled 43 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 15:23:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_35.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_40.html)
2025-03-26 15:23:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_33.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_38.html)
2025-03-26 15:23:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/zhuangxiutupian_34.html> (referer: https://sc.chinaz.com/tupian/zhuangxiutupian_39.html)
2025-03-26 15:24:23 [scrapy.core.engine] INFO: Closing spider (finished)
2025-03-26 15:24:23 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 14482,
 'downloader/request_count': 46,
 'downloader/request_method_count/GET': 46,
 'downloader/response_bytes': 281301,
 'downloader/response_count': 46,
 'downloader/response_status_count/200': 46,
 'dupefilter/filtered': 394,
 'elapsed_time_seconds': 591.573415,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 3, 26, 7, 24, 23, 616103, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 1568570,
 'httpcompression/response_count': 46,
 'items_per_minute': None,
 'log_count/DEBUG': 52,
 'log_count/ERROR': 4,
 'log_count/INFO': 18,
 'request_depth_max': 9,
 'response_received_count': 46,
 'responses_per_minute': None,
 'scheduler/dequeued': 46,
 'scheduler/dequeued/memory': 46,
 'scheduler/enqueued': 46,
 'scheduler/enqueued/memory': 46,
 'spider_exceptions/HTTPError': 4,
 'start_time': datetime.datetime(2025, 3, 26, 7, 14, 32, 42688, tzinfo=datetime.timezone.utc)}
2025-03-26 15:24:23 [scrapy.core.engine] INFO: Spider closed (finished)
2025-03-26 15:28:27 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: yolov8_)
2025-03-26 15:28:27 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.11.7, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Windows-11-10.0.26100-SP0
2025-03-26 15:28:27 [scrapy.addons] INFO: Enabled addons:
[]
2025-03-26 15:28:27 [asyncio] DEBUG: Using selector: SelectSelector
2025-03-26 15:28:27 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2025-03-26 15:28:27 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
2025-03-26 15:28:27 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2025-03-26 15:28:27 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
2025-03-26 15:28:27 [scrapy.extensions.telnet] INFO: Telnet Password: 30e8509a5c63d2fa
2025-03-26 15:28:27 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2025-03-26 15:28:27 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'yolov8_',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'tupian_.log',
 'NEWSPIDER_MODULE': 'yolov8_.spiders',
 'SPIDER_MODULES': ['yolov8_.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2025-03-26 15:28:27 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-03-26 15:28:27 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-03-26 15:28:27 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-03-26 15:28:27 [scrapy.core.engine] INFO: Spider opened
2025-03-26 15:28:27 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 15:28:27 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-03-26 15:28:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian.html> (referer: None)
2025-03-26 15:28:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_2.html> (referer: https://sc.chinaz.com/tupian/meishitupian.html)
2025-03-26 15:28:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_4.html> (referer: https://sc.chinaz.com/tupian/meishitupian.html)
2025-03-26 15:28:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_6.html> (referer: https://sc.chinaz.com/tupian/meishitupian.html)
2025-03-26 15:28:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_3.html> (referer: https://sc.chinaz.com/tupian/meishitupian.html)
2025-03-26 15:28:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_293.html> (referer: https://sc.chinaz.com/tupian/meishitupian.html)
2025-03-26 15:28:41 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://sc.chinaz.com/tupian/meishitupian_3.html> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2025-03-26 15:28:41 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_5.html> (referer: https://sc.chinaz.com/tupian/meishitupian.html)
2025-03-26 15:29:25 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_293.html> (referer: https://sc.chinaz.com/tupian/meishitupian.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-26 15:29:39 [scrapy.extensions.logstats] INFO: Crawled 7 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 15:29:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_7.html> (referer: https://sc.chinaz.com/tupian/meishitupian_2.html)
2025-03-26 15:29:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_8.html> (referer: https://sc.chinaz.com/tupian/meishitupian_4.html)
2025-03-26 15:29:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_9.html> (referer: https://sc.chinaz.com/tupian/meishitupian_4.html)
2025-03-26 15:29:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_10.html> (referer: https://sc.chinaz.com/tupian/meishitupian_6.html)
2025-03-26 15:29:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_11.html> (referer: https://sc.chinaz.com/tupian/meishitupian_6.html)
2025-03-26 15:30:57 [scrapy.extensions.logstats] INFO: Crawled 12 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 15:30:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_14.html> (referer: https://sc.chinaz.com/tupian/meishitupian_11.html)
2025-03-26 15:30:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_13.html> (referer: https://sc.chinaz.com/tupian/meishitupian_11.html)
2025-03-26 15:30:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_12.html> (referer: https://sc.chinaz.com/tupian/meishitupian_11.html)
2025-03-26 15:30:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_15.html> (referer: https://sc.chinaz.com/tupian/meishitupian_11.html)
2025-03-26 15:30:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_16.html> (referer: https://sc.chinaz.com/tupian/meishitupian_11.html)
2025-03-26 15:31:54 [scrapy.extensions.logstats] INFO: Crawled 17 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 15:32:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_17.html> (referer: https://sc.chinaz.com/tupian/meishitupian_16.html)
2025-03-26 15:32:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_18.html> (referer: https://sc.chinaz.com/tupian/meishitupian_16.html)
2025-03-26 15:32:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_19.html> (referer: https://sc.chinaz.com/tupian/meishitupian_16.html)
2025-03-26 15:32:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_21.html> (referer: https://sc.chinaz.com/tupian/meishitupian_16.html)
2025-03-26 15:32:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_20.html> (referer: https://sc.chinaz.com/tupian/meishitupian_16.html)
2025-03-26 15:32:56 [scrapy.extensions.logstats] INFO: Crawled 22 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 15:33:43 [scrapy.extensions.logstats] INFO: Crawled 22 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 15:33:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_249.html> (referer: https://sc.chinaz.com/tupian/meishitupian_21.html)
2025-03-26 15:33:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_25.html> (referer: https://sc.chinaz.com/tupian/meishitupian_21.html)
2025-03-26 15:33:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_26.html> (referer: https://sc.chinaz.com/tupian/meishitupian_21.html)
2025-03-26 15:33:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_24.html> (referer: https://sc.chinaz.com/tupian/meishitupian_21.html)
2025-03-26 15:33:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_23.html> (referer: https://sc.chinaz.com/tupian/meishitupian_21.html)
2025-03-26 15:33:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_22.html> (referer: https://sc.chinaz.com/tupian/meishitupian_21.html)
2025-03-26 15:34:35 [scrapy.extensions.logstats] INFO: Crawled 28 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 15:34:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_23.html> (referer: https://sc.chinaz.com/tupian/meishitupian_21.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 15:34:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_22.html> (referer: https://sc.chinaz.com/tupian/meishitupian_21.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 15:34:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_248.html> (referer: https://sc.chinaz.com/tupian/meishitupian_249.html)
2025-03-26 15:34:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_27.html> (referer: https://sc.chinaz.com/tupian/meishitupian_26.html)
2025-03-26 15:34:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_246.html> (referer: https://sc.chinaz.com/tupian/meishitupian_249.html)
2025-03-26 15:34:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_28.html> (referer: https://sc.chinaz.com/tupian/meishitupian_26.html)
2025-03-26 15:34:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_250.html> (referer: https://sc.chinaz.com/tupian/meishitupian_249.html)
2025-03-26 15:34:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_245.html> (referer: https://sc.chinaz.com/tupian/meishitupian_249.html)
2025-03-26 15:34:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_247.html> (referer: https://sc.chinaz.com/tupian/meishitupian_249.html)
2025-03-26 15:34:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_244.html> (referer: https://sc.chinaz.com/tupian/meishitupian_249.html)
2025-03-26 15:34:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_248.html> (referer: https://sc.chinaz.com/tupian/meishitupian_249.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 15:34:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_269.html> (referer: https://sc.chinaz.com/tupian/meishitupian_249.html)
2025-03-26 15:34:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_29.html> (referer: https://sc.chinaz.com/tupian/meishitupian_26.html)
2025-03-26 15:34:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_31.html> (referer: https://sc.chinaz.com/tupian/meishitupian_26.html)
2025-03-26 15:34:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_254.html> (referer: https://sc.chinaz.com/tupian/meishitupian_249.html)
2025-03-26 15:34:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_252.html> (referer: https://sc.chinaz.com/tupian/meishitupian_249.html)
2025-03-26 15:34:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_251.html> (referer: https://sc.chinaz.com/tupian/meishitupian_249.html)
2025-03-26 15:34:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_253.html> (referer: https://sc.chinaz.com/tupian/meishitupian_249.html)
2025-03-26 15:34:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_30.html> (referer: https://sc.chinaz.com/tupian/meishitupian_26.html)
2025-03-26 15:34:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_27.html> (referer: https://sc.chinaz.com/tupian/meishitupian_26.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 15:34:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_28.html> (referer: https://sc.chinaz.com/tupian/meishitupian_26.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 15:34:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_250.html> (referer: https://sc.chinaz.com/tupian/meishitupian_249.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 15:35:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_245.html> (referer: https://sc.chinaz.com/tupian/meishitupian_249.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 15:35:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_269.html> (referer: https://sc.chinaz.com/tupian/meishitupian_249.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 15:35:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_29.html> (referer: https://sc.chinaz.com/tupian/meishitupian_26.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 15:35:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_31.html> (referer: https://sc.chinaz.com/tupian/meishitupian_26.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 15:35:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_252.html> (referer: https://sc.chinaz.com/tupian/meishitupian_249.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 15:35:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_253.html> (referer: https://sc.chinaz.com/tupian/meishitupian_249.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 15:35:56 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_30.html> (referer: https://sc.chinaz.com/tupian/meishitupian_26.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 15:35:56 [scrapy.extensions.logstats] INFO: Crawled 44 pages (at 16 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 15:35:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_239.html> (referer: https://sc.chinaz.com/tupian/meishitupian_244.html)
2025-03-26 15:35:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_243.html> (referer: https://sc.chinaz.com/tupian/meishitupian_244.html)
2025-03-26 15:35:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_256.html> (referer: https://sc.chinaz.com/tupian/meishitupian_254.html)
2025-03-26 15:35:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_240.html> (referer: https://sc.chinaz.com/tupian/meishitupian_244.html)
2025-03-26 15:35:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_255.html> (referer: https://sc.chinaz.com/tupian/meishitupian_254.html)
2025-03-26 15:35:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_241.html> (referer: https://sc.chinaz.com/tupian/meishitupian_246.html)
2025-03-26 15:35:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_242.html> (referer: https://sc.chinaz.com/tupian/meishitupian_247.html)
2025-03-26 15:35:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_257.html> (referer: https://sc.chinaz.com/tupian/meishitupian_254.html)
2025-03-26 15:36:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_267.html> (referer: https://sc.chinaz.com/tupian/meishitupian_247.html)
2025-03-26 15:36:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_266.html> (referer: https://sc.chinaz.com/tupian/meishitupian_246.html)
2025-03-26 15:36:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_264.html> (referer: https://sc.chinaz.com/tupian/meishitupian_244.html)
2025-03-26 15:36:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_259.html> (referer: https://sc.chinaz.com/tupian/meishitupian_254.html)
2025-03-26 15:36:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_271.html> (referer: https://sc.chinaz.com/tupian/meishitupian_251.html)
2025-03-26 15:36:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_274.html> (referer: https://sc.chinaz.com/tupian/meishitupian_254.html)
2025-03-26 15:36:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_258.html> (referer: https://sc.chinaz.com/tupian/meishitupian_254.html)
2025-03-26 15:37:02 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_242.html> (referer: https://sc.chinaz.com/tupian/meishitupian_247.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 15:37:13 [scrapy.extensions.logstats] INFO: Crawled 59 pages (at 15 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 15:37:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_267.html> (referer: https://sc.chinaz.com/tupian/meishitupian_247.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 15:37:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_271.html> (referer: https://sc.chinaz.com/tupian/meishitupian_251.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 15:38:04 [scrapy.extensions.logstats] INFO: Crawled 59 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 15:38:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_234.html> (referer: https://sc.chinaz.com/tupian/meishitupian_239.html)
2025-03-26 15:38:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_238.html> (referer: https://sc.chinaz.com/tupian/meishitupian_239.html)
2025-03-26 15:38:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_273.html> (referer: https://sc.chinaz.com/tupian/meishitupian_274.html)
2025-03-26 15:38:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_237.html> (referer: https://sc.chinaz.com/tupian/meishitupian_241.html)
2025-03-26 15:38:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_236.html> (referer: https://sc.chinaz.com/tupian/meishitupian_241.html)
2025-03-26 15:38:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_263.html> (referer: https://sc.chinaz.com/tupian/meishitupian_264.html)
2025-03-26 15:38:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_265.html> (referer: https://sc.chinaz.com/tupian/meishitupian_266.html)
2025-03-26 15:38:04 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_235.html> (referer: https://sc.chinaz.com/tupian/meishitupian_239.html)
2025-03-26 15:38:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_270.html> (referer: https://sc.chinaz.com/tupian/meishitupian_274.html)
2025-03-26 15:38:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_261.html> (referer: https://sc.chinaz.com/tupian/meishitupian_266.html)
2025-03-26 15:38:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_272.html> (referer: https://sc.chinaz.com/tupian/meishitupian_274.html)
2025-03-26 15:38:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_260.html> (referer: https://sc.chinaz.com/tupian/meishitupian_264.html)
2025-03-26 15:38:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_275.html> (referer: https://sc.chinaz.com/tupian/meishitupian_274.html)
2025-03-26 15:38:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_262.html> (referer: https://sc.chinaz.com/tupian/meishitupian_266.html)
2025-03-26 15:38:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_276.html> (referer: https://sc.chinaz.com/tupian/meishitupian_274.html)
2025-03-26 15:38:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_268.html> (referer: https://sc.chinaz.com/tupian/meishitupian_266.html)
2025-03-26 15:38:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_273.html> (referer: https://sc.chinaz.com/tupian/meishitupian_274.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 15:39:24 [scrapy.extensions.logstats] INFO: Crawled 75 pages (at 16 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 15:39:24 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_284.html> (referer: https://sc.chinaz.com/tupian/meishitupian_264.html)
2025-03-26 15:39:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_261.html> (referer: https://sc.chinaz.com/tupian/meishitupian_266.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 15:39:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_275.html> (referer: https://sc.chinaz.com/tupian/meishitupian_274.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 15:40:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_268.html> (referer: https://sc.chinaz.com/tupian/meishitupian_266.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 15:40:14 [scrapy.extensions.logstats] INFO: Crawled 76 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 15:40:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_284.html> (referer: https://sc.chinaz.com/tupian/meishitupian_264.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-26 15:40:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_233.html> (referer: https://sc.chinaz.com/tupian/meishitupian_234.html)
2025-03-26 15:40:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_278.html> (referer: https://sc.chinaz.com/tupian/meishitupian_274.html)
2025-03-26 15:40:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_286.html> (referer: https://sc.chinaz.com/tupian/meishitupian_266.html)
2025-03-26 15:40:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_229.html> (referer: https://sc.chinaz.com/tupian/meishitupian_234.html)
2025-03-26 15:40:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_279.html> (referer: https://sc.chinaz.com/tupian/meishitupian_274.html)
2025-03-26 15:40:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_277.html> (referer: https://sc.chinaz.com/tupian/meishitupian_274.html)
2025-03-26 15:40:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_230.html> (referer: https://sc.chinaz.com/tupian/meishitupian_234.html)
2025-03-26 15:40:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_231.html> (referer: https://sc.chinaz.com/tupian/meishitupian_236.html)
2025-03-26 15:40:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_285.html> (referer: https://sc.chinaz.com/tupian/meishitupian_265.html)
2025-03-26 15:40:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_232.html> (referer: https://sc.chinaz.com/tupian/meishitupian_237.html)
2025-03-26 15:40:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_283.html> (referer: https://sc.chinaz.com/tupian/meishitupian_263.html)
2025-03-26 15:40:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_280.html> (referer: https://sc.chinaz.com/tupian/meishitupian_276.html)
2025-03-26 15:40:30 [scrapy.extensions.logstats] INFO: Crawled 88 pages (at 12 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 15:40:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_281.html> (referer: https://sc.chinaz.com/tupian/meishitupian_276.html)
2025-03-26 15:40:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_290.html> (referer: https://sc.chinaz.com/tupian/meishitupian_270.html)
2025-03-26 15:40:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_282.html> (referer: https://sc.chinaz.com/tupian/meishitupian_262.html)
2025-03-26 15:40:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_292.html> (referer: https://sc.chinaz.com/tupian/meishitupian_272.html)
2025-03-26 15:40:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_229.html> (referer: https://sc.chinaz.com/tupian/meishitupian_234.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 15:40:42 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_286.html> (referer: https://sc.chinaz.com/tupian/meishitupian_266.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 15:41:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_285.html> (referer: https://sc.chinaz.com/tupian/meishitupian_265.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-26 15:41:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_232.html> (referer: https://sc.chinaz.com/tupian/meishitupian_237.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 15:41:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_283.html> (referer: https://sc.chinaz.com/tupian/meishitupian_263.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 15:41:57 [scrapy.extensions.logstats] INFO: Crawled 92 pages (at 4 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 15:42:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_290.html> (referer: https://sc.chinaz.com/tupian/meishitupian_270.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 15:42:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_282.html> (referer: https://sc.chinaz.com/tupian/meishitupian_262.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 15:42:13 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_292.html> (referer: https://sc.chinaz.com/tupian/meishitupian_272.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 15:42:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_228.html> (referer: https://sc.chinaz.com/tupian/meishitupian_233.html)
2025-03-26 15:42:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_225.html> (referer: https://sc.chinaz.com/tupian/meishitupian_230.html)
2025-03-26 15:42:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_226.html> (referer: https://sc.chinaz.com/tupian/meishitupian_231.html)
2025-03-26 15:42:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/meishitupian_227.html> (referer: https://sc.chinaz.com/tupian/meishitupian_231.html)
2025-03-26 15:42:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_228.html> (referer: https://sc.chinaz.com/tupian/meishitupian_233.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 15:42:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_225.html> (referer: https://sc.chinaz.com/tupian/meishitupian_230.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 15:42:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_226.html> (referer: https://sc.chinaz.com/tupian/meishitupian_231.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 15:42:14 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/meishitupian_227.html> (referer: https://sc.chinaz.com/tupian/meishitupian_231.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./meishi/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 15:42:14 [scrapy.core.engine] INFO: Closing spider (finished)
2025-03-26 15:42:14 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 29890,
 'downloader/request_count': 96,
 'downloader/request_method_count/GET': 96,
 'downloader/response_bytes': 597987,
 'downloader/response_count': 96,
 'downloader/response_status_count/200': 96,
 'dupefilter/filtered': 567,
 'elapsed_time_seconds': 826.988349,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 3, 26, 7, 42, 14, 632822, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 3393406,
 'httpcompression/response_count': 96,
 'items_per_minute': None,
 'log_count/DEBUG': 102,
 'log_count/ERROR': 34,
 'log_count/INFO': 23,
 'request_depth_max': 10,
 'response_received_count': 96,
 'responses_per_minute': None,
 'scheduler/dequeued': 96,
 'scheduler/dequeued/memory': 96,
 'scheduler/enqueued': 96,
 'scheduler/enqueued/memory': 96,
 'spider_exceptions/HTTPError': 34,
 'start_time': datetime.datetime(2025, 3, 26, 7, 28, 27, 644473, tzinfo=datetime.timezone.utc)}
2025-03-26 15:42:14 [scrapy.core.engine] INFO: Spider closed (finished)
2025-03-26 16:17:24 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: yolov8_)
2025-03-26 16:17:24 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.11.7, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Windows-11-10.0.26100-SP0
2025-03-26 16:17:24 [scrapy.addons] INFO: Enabled addons:
[]
2025-03-26 16:17:24 [asyncio] DEBUG: Using selector: SelectSelector
2025-03-26 16:17:24 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2025-03-26 16:17:24 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
2025-03-26 16:17:24 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2025-03-26 16:17:24 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
2025-03-26 16:17:24 [scrapy.extensions.telnet] INFO: Telnet Password: d91e919aa8c35456
2025-03-26 16:17:24 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2025-03-26 16:17:24 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'yolov8_',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'tupian_.log',
 'NEWSPIDER_MODULE': 'yolov8_.spiders',
 'SPIDER_MODULES': ['yolov8_.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2025-03-26 16:17:24 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-03-26 16:17:24 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-03-26 16:17:24 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-03-26 16:17:24 [scrapy.core.engine] INFO: Spider opened
2025-03-26 16:17:24 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 16:17:24 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-03-26 16:17:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian.html> (referer: None)
2025-03-26 16:17:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_2.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian.html)
2025-03-26 16:17:47 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://sc.chinaz.com/tupian/jianzhutupian_3.html> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2025-03-26 16:17:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_6.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian.html)
2025-03-26 16:17:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_4.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian.html)
2025-03-26 16:17:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_3.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian.html)
2025-03-26 16:17:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_131.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian.html)
2025-03-26 16:17:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_5.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian.html)
2025-03-26 16:18:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/jianzhutupian_131.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./jianzhu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-26 16:18:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_7.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_2.html)
2025-03-26 16:18:37 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 8 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 16:19:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_12.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_7.html)
2025-03-26 16:19:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_10.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_6.html)
2025-03-26 16:19:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_11.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_6.html)
2025-03-26 16:19:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_8.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_6.html)
2025-03-26 16:19:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_9.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_6.html)
2025-03-26 16:20:32 [scrapy.extensions.logstats] INFO: Crawled 13 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 16:20:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_14.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_12.html)
2025-03-26 16:20:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_15.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_12.html)
2025-03-26 16:20:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_13.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_12.html)
2025-03-26 16:20:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_16.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_12.html)
2025-03-26 16:20:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_17.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_12.html)
2025-03-26 16:21:27 [scrapy.extensions.logstats] INFO: Crawled 18 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 16:22:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_22.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_17.html)
2025-03-26 16:22:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_21.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_16.html)
2025-03-26 16:22:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_20.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_15.html)
2025-03-26 16:22:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_18.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_15.html)
2025-03-26 16:22:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_19.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_15.html)
2025-03-26 16:22:54 [scrapy.extensions.logstats] INFO: Crawled 23 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 16:23:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_25.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_22.html)
2025-03-26 16:23:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_24.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_22.html)
2025-03-26 16:23:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_26.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_22.html)
2025-03-26 16:23:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_23.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_22.html)
2025-03-26 16:23:08 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_27.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_22.html)
2025-03-26 16:24:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_119.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_22.html)
2025-03-26 16:24:12 [scrapy.extensions.logstats] INFO: Crawled 29 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 16:24:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_31.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_27.html)
2025-03-26 16:24:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_28.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_27.html)
2025-03-26 16:24:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_30.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_27.html)
2025-03-26 16:24:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_32.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_27.html)
2025-03-26 16:24:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_29.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_27.html)
2025-03-26 16:24:50 [scrapy.extensions.logstats] INFO: Crawled 34 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 16:24:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_121.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_119.html)
2025-03-26 16:24:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_114.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_119.html)
2025-03-26 16:24:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_120.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_119.html)
2025-03-26 16:24:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_118.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_119.html)
2025-03-26 16:24:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_115.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_119.html)
2025-03-26 16:24:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_122.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_119.html)
2025-03-26 16:24:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_116.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_119.html)
2025-03-26 16:24:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_117.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_119.html)
2025-03-26 16:25:25 [scrapy.extensions.logstats] INFO: Crawled 42 pages (at 8 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 16:27:22 [scrapy.extensions.logstats] INFO: Crawled 42 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 16:27:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_123.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_119.html)
2025-03-26 16:27:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_124.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_119.html)
2025-03-26 16:27:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_113.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_114.html)
2025-03-26 16:27:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_111.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_116.html)
2025-03-26 16:27:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_33.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_31.html)
2025-03-26 16:27:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_112.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_117.html)
2025-03-26 16:27:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_109.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_114.html)
2025-03-26 16:27:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_110.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_115.html)
2025-03-26 16:27:45 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/jianzhutupian_124.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_119.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./jianzhu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-26 16:27:45 [scrapy.extensions.logstats] INFO: Crawled 50 pages (at 8 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 16:27:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_126.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_122.html)
2025-03-26 16:27:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_37.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_32.html)
2025-03-26 16:27:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_34.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_31.html)
2025-03-26 16:27:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_125.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_122.html)
2025-03-26 16:27:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_35.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_31.html)
2025-03-26 16:27:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_36.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_31.html)
2025-03-26 16:27:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_127.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_122.html)
2025-03-26 16:29:03 [scrapy.extensions.logstats] INFO: Crawled 57 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 16:29:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/jianzhutupian_126.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_122.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./jianzhu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-26 16:29:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/jianzhutupian_127.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_122.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./jianzhu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-26 16:29:38 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/jianzhutupian_125.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_122.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./jianzhu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-26 16:29:49 [scrapy.extensions.logstats] INFO: Crawled 57 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 16:29:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_108.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_109.html)
2025-03-26 16:29:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_107.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_112.html)
2025-03-26 16:29:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_104.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_109.html)
2025-03-26 16:29:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_106.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_111.html)
2025-03-26 16:29:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_105.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_110.html)
2025-03-26 16:29:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_128.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_123.html)
2025-03-26 16:29:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_39.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_37.html)
2025-03-26 16:29:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_38.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_37.html)
2025-03-26 16:29:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_40.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_37.html)
2025-03-26 16:29:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_130.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_110.html)
2025-03-26 16:29:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_42.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_37.html)
2025-03-26 16:29:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_129.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_109.html)
2025-03-26 16:29:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_41.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_37.html)
2025-03-26 16:30:35 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/jianzhutupian_128.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_123.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./jianzhu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-26 16:30:59 [scrapy.extensions.logstats] INFO: Crawled 70 pages (at 13 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 16:31:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/jianzhutupian_130.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_110.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./jianzhu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-26 16:31:24 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/jianzhutupian_129.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_109.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./jianzhu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-26 16:31:36 [scrapy.extensions.logstats] INFO: Crawled 70 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 16:31:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_103.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_108.html)
2025-03-26 16:31:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_99.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_104.html)
2025-03-26 16:31:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_102.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_107.html)
2025-03-26 16:31:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_100.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_105.html)
2025-03-26 16:31:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_43.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_42.html)
2025-03-26 16:31:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_101.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_106.html)
2025-03-26 16:31:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_44.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_42.html)
2025-03-26 16:31:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_47.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_42.html)
2025-03-26 16:31:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_45.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_42.html)
2025-03-26 16:31:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_46.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_42.html)
2025-03-26 16:32:37 [scrapy.extensions.logstats] INFO: Crawled 80 pages (at 10 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 16:33:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_98.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_103.html)
2025-03-26 16:33:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_97.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_102.html)
2025-03-26 16:33:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_94.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_99.html)
2025-03-26 16:33:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_95.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_100.html)
2025-03-26 16:33:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_96.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_101.html)
2025-03-26 16:33:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_50.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_47.html)
2025-03-26 16:33:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_49.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_47.html)
2025-03-26 16:33:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_48.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_47.html)
2025-03-26 16:33:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_51.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_47.html)
2025-03-26 16:33:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_52.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_47.html)
2025-03-26 16:33:25 [scrapy.extensions.logstats] INFO: Crawled 90 pages (at 10 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 16:35:04 [scrapy.extensions.logstats] INFO: Crawled 90 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 16:35:35 [scrapy.extensions.logstats] INFO: Crawled 90 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 16:35:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_93.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_98.html)
2025-03-26 16:35:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_91.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_96.html)
2025-03-26 16:35:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_92.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_97.html)
2025-03-26 16:35:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_89.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_94.html)
2025-03-26 16:35:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_55.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_50.html)
2025-03-26 16:35:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_90.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_95.html)
2025-03-26 16:35:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_54.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_50.html)
2025-03-26 16:35:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_53.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_50.html)
2025-03-26 16:35:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_57.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_52.html)
2025-03-26 16:35:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_56.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_52.html)
2025-03-26 16:37:22 [scrapy.extensions.logstats] INFO: Crawled 100 pages (at 10 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 16:37:49 [scrapy.extensions.logstats] INFO: Crawled 100 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 16:37:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_88.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_93.html)
2025-03-26 16:37:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_85.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_90.html)
2025-03-26 16:37:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_87.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_92.html)
2025-03-26 16:37:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_86.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_91.html)
2025-03-26 16:37:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_58.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_55.html)
2025-03-26 16:37:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_60.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_55.html)
2025-03-26 16:37:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_59.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_55.html)
2025-03-26 16:38:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_61.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_57.html)
2025-03-26 16:38:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_62.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_57.html)
2025-03-26 16:38:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_84.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_89.html)
2025-03-26 16:39:30 [scrapy.extensions.logstats] INFO: Crawled 110 pages (at 10 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 16:40:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_83.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_88.html)
2025-03-26 16:40:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_82.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_87.html)
2025-03-26 16:40:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_80.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_85.html)
2025-03-26 16:40:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_64.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_62.html)
2025-03-26 16:40:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_79.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_84.html)
2025-03-26 16:40:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_63.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_62.html)
2025-03-26 16:40:12 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_65.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_62.html)
2025-03-26 16:40:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_66.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_62.html)
2025-03-26 16:40:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_81.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_86.html)
2025-03-26 16:40:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_67.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_62.html)
2025-03-26 16:41:39 [scrapy.extensions.logstats] INFO: Crawled 120 pages (at 10 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 16:42:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_78.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_83.html)
2025-03-26 16:42:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_74.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_79.html)
2025-03-26 16:42:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_68.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_65.html)
2025-03-26 16:42:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_76.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_80.html)
2025-03-26 16:42:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_77.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_82.html)
2025-03-26 16:42:35 [scrapy.extensions.logstats] INFO: Crawled 125 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 16:42:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_72.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_67.html)
2025-03-26 16:42:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_70.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_65.html)
2025-03-26 16:42:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_75.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_80.html)
2025-03-26 16:42:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_69.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_65.html)
2025-03-26 16:42:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_71.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_67.html)
2025-03-26 16:44:28 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/jianzhutupian_73.html> (referer: https://sc.chinaz.com/tupian/jianzhutupian_78.html)
2025-03-26 16:44:28 [scrapy.extensions.logstats] INFO: Crawled 131 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 16:44:38 [scrapy.core.engine] INFO: Closing spider (finished)
2025-03-26 16:44:38 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 40974,
 'downloader/request_count': 131,
 'downloader/request_method_count/GET': 131,
 'downloader/response_bytes': 811788,
 'downloader/response_count': 131,
 'downloader/response_status_count/200': 131,
 'dupefilter/filtered': 1203,
 'elapsed_time_seconds': 1633.834772,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 3, 26, 8, 44, 38, 737184, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 4520634,
 'httpcompression/response_count': 131,
 'items_per_minute': None,
 'log_count/DEBUG': 137,
 'log_count/ERROR': 8,
 'log_count/INFO': 33,
 'request_depth_max': 17,
 'response_received_count': 131,
 'responses_per_minute': None,
 'scheduler/dequeued': 131,
 'scheduler/dequeued/memory': 131,
 'scheduler/enqueued': 131,
 'scheduler/enqueued/memory': 131,
 'spider_exceptions/HTTPError': 8,
 'start_time': datetime.datetime(2025, 3, 26, 8, 17, 24, 902412, tzinfo=datetime.timezone.utc)}
2025-03-26 16:44:38 [scrapy.core.engine] INFO: Spider closed (finished)
2025-03-26 16:50:38 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: yolov8_)
2025-03-26 16:50:39 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.11.7, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Windows-11-10.0.26100-SP0
2025-03-26 16:50:39 [scrapy.addons] INFO: Enabled addons:
[]
2025-03-26 16:50:39 [asyncio] DEBUG: Using selector: SelectSelector
2025-03-26 16:50:39 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2025-03-26 16:50:39 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
2025-03-26 16:50:39 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2025-03-26 16:50:39 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
2025-03-26 16:50:39 [scrapy.extensions.telnet] INFO: Telnet Password: a433025d4157882b
2025-03-26 16:50:39 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2025-03-26 16:50:39 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'yolov8_',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'tupian_.log',
 'NEWSPIDER_MODULE': 'yolov8_.spiders',
 'SPIDER_MODULES': ['yolov8_.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2025-03-26 16:50:39 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-03-26 16:50:39 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-03-26 16:50:39 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-03-26 16:50:39 [scrapy.core.engine] INFO: Spider opened
2025-03-26 16:50:39 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 16:50:39 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-03-26 16:50:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian.html> (referer: None)
2025-03-26 16:50:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_2.html> (referer: https://sc.chinaz.com/tupian/renwutupian.html)
2025-03-26 16:50:59 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://sc.chinaz.com/tupian/renwutupian_3.html> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2025-03-26 16:50:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_4.html> (referer: https://sc.chinaz.com/tupian/renwutupian.html)
2025-03-26 16:50:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_1094.html> (referer: https://sc.chinaz.com/tupian/renwutupian.html)
2025-03-26 16:50:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_5.html> (referer: https://sc.chinaz.com/tupian/renwutupian.html)
2025-03-26 16:50:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_6.html> (referer: https://sc.chinaz.com/tupian/renwutupian.html)
2025-03-26 16:50:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_3.html> (referer: https://sc.chinaz.com/tupian/renwutupian.html)
2025-03-26 16:51:15 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_1094.html> (referer: https://sc.chinaz.com/tupian/renwutupian.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-26 16:51:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_7.html> (referer: https://sc.chinaz.com/tupian/renwutupian_2.html)
2025-03-26 16:52:05 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 8 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 16:52:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_8.html> (referer: https://sc.chinaz.com/tupian/renwutupian_4.html)
2025-03-26 16:52:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_10.html> (referer: https://sc.chinaz.com/tupian/renwutupian_5.html)
2025-03-26 16:52:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_9.html> (referer: https://sc.chinaz.com/tupian/renwutupian_4.html)
2025-03-26 16:52:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_11.html> (referer: https://sc.chinaz.com/tupian/renwutupian_6.html)
2025-03-26 16:52:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_12.html> (referer: https://sc.chinaz.com/tupian/renwutupian_7.html)
2025-03-26 16:52:39 [scrapy.extensions.logstats] INFO: Crawled 13 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 16:53:44 [scrapy.extensions.logstats] INFO: Crawled 13 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 16:53:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_14.html> (referer: https://sc.chinaz.com/tupian/renwutupian_12.html)
2025-03-26 16:53:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_13.html> (referer: https://sc.chinaz.com/tupian/renwutupian_12.html)
2025-03-26 16:53:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_15.html> (referer: https://sc.chinaz.com/tupian/renwutupian_12.html)
2025-03-26 16:53:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_17.html> (referer: https://sc.chinaz.com/tupian/renwutupian_12.html)
2025-03-26 16:53:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_16.html> (referer: https://sc.chinaz.com/tupian/renwutupian_12.html)
2025-03-26 16:55:16 [scrapy.extensions.logstats] INFO: Crawled 18 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 16:55:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_19.html> (referer: https://sc.chinaz.com/tupian/renwutupian_17.html)
2025-03-26 16:55:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_18.html> (referer: https://sc.chinaz.com/tupian/renwutupian_17.html)
2025-03-26 16:55:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_20.html> (referer: https://sc.chinaz.com/tupian/renwutupian_17.html)
2025-03-26 16:55:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_21.html> (referer: https://sc.chinaz.com/tupian/renwutupian_17.html)
2025-03-26 16:55:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_22.html> (referer: https://sc.chinaz.com/tupian/renwutupian_17.html)
2025-03-26 16:56:49 [scrapy.extensions.logstats] INFO: Crawled 23 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 16:56:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_26.html> (referer: https://sc.chinaz.com/tupian/renwutupian_22.html)
2025-03-26 16:56:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_25.html> (referer: https://sc.chinaz.com/tupian/renwutupian_22.html)
2025-03-26 16:56:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_878.html> (referer: https://sc.chinaz.com/tupian/renwutupian_22.html)
2025-03-26 16:56:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_24.html> (referer: https://sc.chinaz.com/tupian/renwutupian_22.html)
2025-03-26 16:56:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_23.html> (referer: https://sc.chinaz.com/tupian/renwutupian_22.html)
2025-03-26 16:56:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_27.html> (referer: https://sc.chinaz.com/tupian/renwutupian_22.html)
2025-03-26 16:57:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_878.html> (referer: https://sc.chinaz.com/tupian/renwutupian_22.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-26 16:58:18 [scrapy.extensions.logstats] INFO: Crawled 29 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 16:58:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_28.html> (referer: https://sc.chinaz.com/tupian/renwutupian_26.html)
2025-03-26 16:58:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_29.html> (referer: https://sc.chinaz.com/tupian/renwutupian_26.html)
2025-03-26 16:58:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_31.html> (referer: https://sc.chinaz.com/tupian/renwutupian_26.html)
2025-03-26 16:58:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_32.html> (referer: https://sc.chinaz.com/tupian/renwutupian_27.html)
2025-03-26 16:58:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_30.html> (referer: https://sc.chinaz.com/tupian/renwutupian_26.html)
2025-03-26 16:58:53 [scrapy.extensions.logstats] INFO: Crawled 34 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 16:59:48 [scrapy.extensions.logstats] INFO: Crawled 34 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 16:59:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_34.html> (referer: https://sc.chinaz.com/tupian/renwutupian_32.html)
2025-03-26 16:59:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_36.html> (referer: https://sc.chinaz.com/tupian/renwutupian_32.html)
2025-03-26 16:59:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_37.html> (referer: https://sc.chinaz.com/tupian/renwutupian_32.html)
2025-03-26 16:59:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_35.html> (referer: https://sc.chinaz.com/tupian/renwutupian_32.html)
2025-03-26 16:59:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_33.html> (referer: https://sc.chinaz.com/tupian/renwutupian_32.html)
2025-03-26 17:01:27 [scrapy.extensions.logstats] INFO: Crawled 39 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:01:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_39.html> (referer: https://sc.chinaz.com/tupian/renwutupian_37.html)
2025-03-26 17:01:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_38.html> (referer: https://sc.chinaz.com/tupian/renwutupian_37.html)
2025-03-26 17:01:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_41.html> (referer: https://sc.chinaz.com/tupian/renwutupian_37.html)
2025-03-26 17:01:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_40.html> (referer: https://sc.chinaz.com/tupian/renwutupian_37.html)
2025-03-26 17:01:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_42.html> (referer: https://sc.chinaz.com/tupian/renwutupian_37.html)
2025-03-26 17:02:02 [scrapy.extensions.logstats] INFO: Crawled 44 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:02:51 [scrapy.extensions.logstats] INFO: Crawled 44 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:02:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_43.html> (referer: https://sc.chinaz.com/tupian/renwutupian_41.html)
2025-03-26 17:02:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_44.html> (referer: https://sc.chinaz.com/tupian/renwutupian_41.html)
2025-03-26 17:02:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_45.html> (referer: https://sc.chinaz.com/tupian/renwutupian_41.html)
2025-03-26 17:02:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_46.html> (referer: https://sc.chinaz.com/tupian/renwutupian_41.html)
2025-03-26 17:03:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_47.html> (referer: https://sc.chinaz.com/tupian/renwutupian_42.html)
2025-03-26 17:03:48 [scrapy.extensions.logstats] INFO: Crawled 49 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:04:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_48.html> (referer: https://sc.chinaz.com/tupian/renwutupian_46.html)
2025-03-26 17:04:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_50.html> (referer: https://sc.chinaz.com/tupian/renwutupian_46.html)
2025-03-26 17:04:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_51.html> (referer: https://sc.chinaz.com/tupian/renwutupian_46.html)
2025-03-26 17:04:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_49.html> (referer: https://sc.chinaz.com/tupian/renwutupian_46.html)
2025-03-26 17:04:52 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_52.html> (referer: https://sc.chinaz.com/tupian/renwutupian_47.html)
2025-03-26 17:04:52 [scrapy.extensions.logstats] INFO: Crawled 54 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:05:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_54.html> (referer: https://sc.chinaz.com/tupian/renwutupian_51.html)
2025-03-26 17:05:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_55.html> (referer: https://sc.chinaz.com/tupian/renwutupian_51.html)
2025-03-26 17:05:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_56.html> (referer: https://sc.chinaz.com/tupian/renwutupian_51.html)
2025-03-26 17:05:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_53.html> (referer: https://sc.chinaz.com/tupian/renwutupian_51.html)
2025-03-26 17:05:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_57.html> (referer: https://sc.chinaz.com/tupian/renwutupian_52.html)
2025-03-26 17:05:49 [scrapy.extensions.logstats] INFO: Crawled 59 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:07:04 [scrapy.extensions.logstats] INFO: Crawled 59 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:07:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_58.html> (referer: https://sc.chinaz.com/tupian/renwutupian_57.html)
2025-03-26 17:07:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_60.html> (referer: https://sc.chinaz.com/tupian/renwutupian_57.html)
2025-03-26 17:07:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_62.html> (referer: https://sc.chinaz.com/tupian/renwutupian_57.html)
2025-03-26 17:07:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_59.html> (referer: https://sc.chinaz.com/tupian/renwutupian_57.html)
2025-03-26 17:07:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_61.html> (referer: https://sc.chinaz.com/tupian/renwutupian_57.html)
2025-03-26 17:08:01 [scrapy.extensions.logstats] INFO: Crawled 64 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:08:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_67.html> (referer: https://sc.chinaz.com/tupian/renwutupian_62.html)
2025-03-26 17:08:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_63.html> (referer: https://sc.chinaz.com/tupian/renwutupian_62.html)
2025-03-26 17:08:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_66.html> (referer: https://sc.chinaz.com/tupian/renwutupian_62.html)
2025-03-26 17:08:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_64.html> (referer: https://sc.chinaz.com/tupian/renwutupian_62.html)
2025-03-26 17:08:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_65.html> (referer: https://sc.chinaz.com/tupian/renwutupian_62.html)
2025-03-26 17:08:48 [scrapy.extensions.logstats] INFO: Crawled 69 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:10:09 [scrapy.extensions.logstats] INFO: Crawled 69 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:10:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_68.html> (referer: https://sc.chinaz.com/tupian/renwutupian_67.html)
2025-03-26 17:10:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_71.html> (referer: https://sc.chinaz.com/tupian/renwutupian_67.html)
2025-03-26 17:10:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_72.html> (referer: https://sc.chinaz.com/tupian/renwutupian_67.html)
2025-03-26 17:10:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_70.html> (referer: https://sc.chinaz.com/tupian/renwutupian_67.html)
2025-03-26 17:10:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_69.html> (referer: https://sc.chinaz.com/tupian/renwutupian_67.html)
2025-03-26 17:11:44 [scrapy.extensions.logstats] INFO: Crawled 74 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:11:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_77.html> (referer: https://sc.chinaz.com/tupian/renwutupian_72.html)
2025-03-26 17:11:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_73.html> (referer: https://sc.chinaz.com/tupian/renwutupian_72.html)
2025-03-26 17:11:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_76.html> (referer: https://sc.chinaz.com/tupian/renwutupian_72.html)
2025-03-26 17:11:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_75.html> (referer: https://sc.chinaz.com/tupian/renwutupian_72.html)
2025-03-26 17:11:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_74.html> (referer: https://sc.chinaz.com/tupian/renwutupian_72.html)
2025-03-26 17:12:50 [scrapy.extensions.logstats] INFO: Crawled 79 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:13:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_78.html> (referer: https://sc.chinaz.com/tupian/renwutupian_77.html)
2025-03-26 17:13:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_79.html> (referer: https://sc.chinaz.com/tupian/renwutupian_77.html)
2025-03-26 17:13:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_82.html> (referer: https://sc.chinaz.com/tupian/renwutupian_77.html)
2025-03-26 17:13:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_81.html> (referer: https://sc.chinaz.com/tupian/renwutupian_77.html)
2025-03-26 17:13:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_80.html> (referer: https://sc.chinaz.com/tupian/renwutupian_77.html)
2025-03-26 17:13:46 [scrapy.extensions.logstats] INFO: Crawled 84 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:14:39 [scrapy.extensions.logstats] INFO: Crawled 84 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:14:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_83.html> (referer: https://sc.chinaz.com/tupian/renwutupian_82.html)
2025-03-26 17:14:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_85.html> (referer: https://sc.chinaz.com/tupian/renwutupian_82.html)
2025-03-26 17:14:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_84.html> (referer: https://sc.chinaz.com/tupian/renwutupian_82.html)
2025-03-26 17:14:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_86.html> (referer: https://sc.chinaz.com/tupian/renwutupian_82.html)
2025-03-26 17:14:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_87.html> (referer: https://sc.chinaz.com/tupian/renwutupian_82.html)
2025-03-26 17:16:09 [scrapy.extensions.logstats] INFO: Crawled 89 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:16:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_89.html> (referer: https://sc.chinaz.com/tupian/renwutupian_87.html)
2025-03-26 17:16:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_88.html> (referer: https://sc.chinaz.com/tupian/renwutupian_87.html)
2025-03-26 17:16:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_92.html> (referer: https://sc.chinaz.com/tupian/renwutupian_87.html)
2025-03-26 17:16:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_90.html> (referer: https://sc.chinaz.com/tupian/renwutupian_87.html)
2025-03-26 17:16:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_91.html> (referer: https://sc.chinaz.com/tupian/renwutupian_87.html)
2025-03-26 17:16:48 [scrapy.extensions.logstats] INFO: Crawled 94 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:17:47 [scrapy.extensions.logstats] INFO: Crawled 94 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:17:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_94.html> (referer: https://sc.chinaz.com/tupian/renwutupian_92.html)
2025-03-26 17:17:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_97.html> (referer: https://sc.chinaz.com/tupian/renwutupian_92.html)
2025-03-26 17:17:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_96.html> (referer: https://sc.chinaz.com/tupian/renwutupian_92.html)
2025-03-26 17:17:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_93.html> (referer: https://sc.chinaz.com/tupian/renwutupian_92.html)
2025-03-26 17:17:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_95.html> (referer: https://sc.chinaz.com/tupian/renwutupian_92.html)
2025-03-26 17:19:18 [scrapy.extensions.logstats] INFO: Crawled 99 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:19:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_98.html> (referer: https://sc.chinaz.com/tupian/renwutupian_97.html)
2025-03-26 17:19:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_101.html> (referer: https://sc.chinaz.com/tupian/renwutupian_97.html)
2025-03-26 17:19:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_102.html> (referer: https://sc.chinaz.com/tupian/renwutupian_97.html)
2025-03-26 17:19:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_99.html> (referer: https://sc.chinaz.com/tupian/renwutupian_97.html)
2025-03-26 17:19:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_100.html> (referer: https://sc.chinaz.com/tupian/renwutupian_97.html)
2025-03-26 17:20:41 [scrapy.extensions.logstats] INFO: Crawled 104 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:20:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_106.html> (referer: https://sc.chinaz.com/tupian/renwutupian_102.html)
2025-03-26 17:20:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_103.html> (referer: https://sc.chinaz.com/tupian/renwutupian_102.html)
2025-03-26 17:20:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_105.html> (referer: https://sc.chinaz.com/tupian/renwutupian_102.html)
2025-03-26 17:20:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_104.html> (referer: https://sc.chinaz.com/tupian/renwutupian_102.html)
2025-03-26 17:20:42 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_107.html> (referer: https://sc.chinaz.com/tupian/renwutupian_102.html)
2025-03-26 17:22:16 [scrapy.extensions.logstats] INFO: Crawled 109 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:22:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_109.html> (referer: https://sc.chinaz.com/tupian/renwutupian_106.html)
2025-03-26 17:22:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_112.html> (referer: https://sc.chinaz.com/tupian/renwutupian_107.html)
2025-03-26 17:22:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_108.html> (referer: https://sc.chinaz.com/tupian/renwutupian_106.html)
2025-03-26 17:22:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_110.html> (referer: https://sc.chinaz.com/tupian/renwutupian_106.html)
2025-03-26 17:22:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_111.html> (referer: https://sc.chinaz.com/tupian/renwutupian_106.html)
2025-03-26 17:23:23 [scrapy.extensions.logstats] INFO: Crawled 114 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:24:01 [scrapy.extensions.logstats] INFO: Crawled 114 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:24:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_114.html> (referer: https://sc.chinaz.com/tupian/renwutupian_112.html)
2025-03-26 17:24:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_113.html> (referer: https://sc.chinaz.com/tupian/renwutupian_112.html)
2025-03-26 17:24:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_116.html> (referer: https://sc.chinaz.com/tupian/renwutupian_112.html)
2025-03-26 17:24:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_117.html> (referer: https://sc.chinaz.com/tupian/renwutupian_112.html)
2025-03-26 17:24:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_115.html> (referer: https://sc.chinaz.com/tupian/renwutupian_112.html)
2025-03-26 17:24:44 [scrapy.extensions.logstats] INFO: Crawled 119 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:25:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_118.html> (referer: https://sc.chinaz.com/tupian/renwutupian_116.html)
2025-03-26 17:25:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_119.html> (referer: https://sc.chinaz.com/tupian/renwutupian_116.html)
2025-03-26 17:25:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_122.html> (referer: https://sc.chinaz.com/tupian/renwutupian_117.html)
2025-03-26 17:25:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_121.html> (referer: https://sc.chinaz.com/tupian/renwutupian_116.html)
2025-03-26 17:25:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_120.html> (referer: https://sc.chinaz.com/tupian/renwutupian_116.html)
2025-03-26 17:25:42 [scrapy.extensions.logstats] INFO: Crawled 124 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:26:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_124.html> (referer: https://sc.chinaz.com/tupian/renwutupian_122.html)
2025-03-26 17:26:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_125.html> (referer: https://sc.chinaz.com/tupian/renwutupian_122.html)
2025-03-26 17:26:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_126.html> (referer: https://sc.chinaz.com/tupian/renwutupian_122.html)
2025-03-26 17:26:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_123.html> (referer: https://sc.chinaz.com/tupian/renwutupian_122.html)
2025-03-26 17:26:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_127.html> (referer: https://sc.chinaz.com/tupian/renwutupian_122.html)
2025-03-26 17:27:29 [scrapy.extensions.logstats] INFO: Crawled 129 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:27:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_128.html> (referer: https://sc.chinaz.com/tupian/renwutupian_127.html)
2025-03-26 17:27:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_130.html> (referer: https://sc.chinaz.com/tupian/renwutupian_127.html)
2025-03-26 17:27:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_129.html> (referer: https://sc.chinaz.com/tupian/renwutupian_127.html)
2025-03-26 17:27:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_132.html> (referer: https://sc.chinaz.com/tupian/renwutupian_127.html)
2025-03-26 17:27:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_131.html> (referer: https://sc.chinaz.com/tupian/renwutupian_127.html)
2025-03-26 17:28:09 [scrapy.extensions.logstats] INFO: Crawled 134 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:28:40 [scrapy.extensions.logstats] INFO: Crawled 134 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:28:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_133.html> (referer: https://sc.chinaz.com/tupian/renwutupian_130.html)
2025-03-26 17:28:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_137.html> (referer: https://sc.chinaz.com/tupian/renwutupian_132.html)
2025-03-26 17:28:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_134.html> (referer: https://sc.chinaz.com/tupian/renwutupian_130.html)
2025-03-26 17:28:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_136.html> (referer: https://sc.chinaz.com/tupian/renwutupian_132.html)
2025-03-26 17:28:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_135.html> (referer: https://sc.chinaz.com/tupian/renwutupian_130.html)
2025-03-26 17:29:58 [scrapy.extensions.logstats] INFO: Crawled 139 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:29:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_138.html> (referer: https://sc.chinaz.com/tupian/renwutupian_137.html)
2025-03-26 17:29:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_142.html> (referer: https://sc.chinaz.com/tupian/renwutupian_137.html)
2025-03-26 17:29:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_140.html> (referer: https://sc.chinaz.com/tupian/renwutupian_137.html)
2025-03-26 17:29:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_139.html> (referer: https://sc.chinaz.com/tupian/renwutupian_137.html)
2025-03-26 17:29:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_141.html> (referer: https://sc.chinaz.com/tupian/renwutupian_137.html)
2025-03-26 17:31:24 [scrapy.extensions.logstats] INFO: Crawled 144 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:31:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_143.html> (referer: https://sc.chinaz.com/tupian/renwutupian_142.html)
2025-03-26 17:31:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_144.html> (referer: https://sc.chinaz.com/tupian/renwutupian_142.html)
2025-03-26 17:31:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_145.html> (referer: https://sc.chinaz.com/tupian/renwutupian_142.html)
2025-03-26 17:31:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_146.html> (referer: https://sc.chinaz.com/tupian/renwutupian_142.html)
2025-03-26 17:31:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_147.html> (referer: https://sc.chinaz.com/tupian/renwutupian_142.html)
2025-03-26 17:32:32 [scrapy.extensions.logstats] INFO: Crawled 149 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:32:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_152.html> (referer: https://sc.chinaz.com/tupian/renwutupian_147.html)
2025-03-26 17:32:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_150.html> (referer: https://sc.chinaz.com/tupian/renwutupian_147.html)
2025-03-26 17:32:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_151.html> (referer: https://sc.chinaz.com/tupian/renwutupian_147.html)
2025-03-26 17:32:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_149.html> (referer: https://sc.chinaz.com/tupian/renwutupian_147.html)
2025-03-26 17:32:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_148.html> (referer: https://sc.chinaz.com/tupian/renwutupian_147.html)
2025-03-26 17:32:44 [scrapy.extensions.logstats] INFO: Crawled 154 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:33:47 [scrapy.extensions.logstats] INFO: Crawled 154 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:33:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_154.html> (referer: https://sc.chinaz.com/tupian/renwutupian_152.html)
2025-03-26 17:33:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_153.html> (referer: https://sc.chinaz.com/tupian/renwutupian_152.html)
2025-03-26 17:33:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_157.html> (referer: https://sc.chinaz.com/tupian/renwutupian_152.html)
2025-03-26 17:33:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_155.html> (referer: https://sc.chinaz.com/tupian/renwutupian_152.html)
2025-03-26 17:33:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_156.html> (referer: https://sc.chinaz.com/tupian/renwutupian_152.html)
2025-03-26 17:35:00 [scrapy.extensions.logstats] INFO: Crawled 159 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:35:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_160.html> (referer: https://sc.chinaz.com/tupian/renwutupian_157.html)
2025-03-26 17:35:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_159.html> (referer: https://sc.chinaz.com/tupian/renwutupian_157.html)
2025-03-26 17:35:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_158.html> (referer: https://sc.chinaz.com/tupian/renwutupian_157.html)
2025-03-26 17:35:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_161.html> (referer: https://sc.chinaz.com/tupian/renwutupian_157.html)
2025-03-26 17:35:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_162.html> (referer: https://sc.chinaz.com/tupian/renwutupian_157.html)
2025-03-26 17:35:56 [scrapy.extensions.logstats] INFO: Crawled 164 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:36:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_164.html> (referer: https://sc.chinaz.com/tupian/renwutupian_161.html)
2025-03-26 17:36:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_163.html> (referer: https://sc.chinaz.com/tupian/renwutupian_161.html)
2025-03-26 17:36:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_165.html> (referer: https://sc.chinaz.com/tupian/renwutupian_161.html)
2025-03-26 17:36:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_167.html> (referer: https://sc.chinaz.com/tupian/renwutupian_162.html)
2025-03-26 17:36:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_166.html> (referer: https://sc.chinaz.com/tupian/renwutupian_161.html)
2025-03-26 17:36:46 [scrapy.extensions.logstats] INFO: Crawled 169 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:37:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_170.html> (referer: https://sc.chinaz.com/tupian/renwutupian_167.html)
2025-03-26 17:37:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_168.html> (referer: https://sc.chinaz.com/tupian/renwutupian_167.html)
2025-03-26 17:37:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_169.html> (referer: https://sc.chinaz.com/tupian/renwutupian_167.html)
2025-03-26 17:37:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_172.html> (referer: https://sc.chinaz.com/tupian/renwutupian_167.html)
2025-03-26 17:37:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_171.html> (referer: https://sc.chinaz.com/tupian/renwutupian_167.html)
2025-03-26 17:37:43 [scrapy.extensions.logstats] INFO: Crawled 174 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:38:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_176.html> (referer: https://sc.chinaz.com/tupian/renwutupian_172.html)
2025-03-26 17:38:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_174.html> (referer: https://sc.chinaz.com/tupian/renwutupian_170.html)
2025-03-26 17:38:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_173.html> (referer: https://sc.chinaz.com/tupian/renwutupian_170.html)
2025-03-26 17:38:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_175.html> (referer: https://sc.chinaz.com/tupian/renwutupian_170.html)
2025-03-26 17:38:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_177.html> (referer: https://sc.chinaz.com/tupian/renwutupian_172.html)
2025-03-26 17:39:04 [scrapy.extensions.logstats] INFO: Crawled 179 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:39:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_178.html> (referer: https://sc.chinaz.com/tupian/renwutupian_176.html)
2025-03-26 17:39:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_182.html> (referer: https://sc.chinaz.com/tupian/renwutupian_177.html)
2025-03-26 17:39:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_179.html> (referer: https://sc.chinaz.com/tupian/renwutupian_176.html)
2025-03-26 17:39:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_181.html> (referer: https://sc.chinaz.com/tupian/renwutupian_176.html)
2025-03-26 17:39:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_180.html> (referer: https://sc.chinaz.com/tupian/renwutupian_176.html)
2025-03-26 17:39:51 [scrapy.extensions.logstats] INFO: Crawled 184 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:40:49 [scrapy.extensions.logstats] INFO: Crawled 184 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:40:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_184.html> (referer: https://sc.chinaz.com/tupian/renwutupian_182.html)
2025-03-26 17:40:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_187.html> (referer: https://sc.chinaz.com/tupian/renwutupian_182.html)
2025-03-26 17:40:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_186.html> (referer: https://sc.chinaz.com/tupian/renwutupian_182.html)
2025-03-26 17:40:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_185.html> (referer: https://sc.chinaz.com/tupian/renwutupian_182.html)
2025-03-26 17:40:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_183.html> (referer: https://sc.chinaz.com/tupian/renwutupian_182.html)
2025-03-26 17:41:42 [scrapy.extensions.logstats] INFO: Crawled 189 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:41:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_188.html> (referer: https://sc.chinaz.com/tupian/renwutupian_187.html)
2025-03-26 17:41:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_189.html> (referer: https://sc.chinaz.com/tupian/renwutupian_187.html)
2025-03-26 17:41:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_190.html> (referer: https://sc.chinaz.com/tupian/renwutupian_187.html)
2025-03-26 17:41:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_192.html> (referer: https://sc.chinaz.com/tupian/renwutupian_187.html)
2025-03-26 17:41:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_191.html> (referer: https://sc.chinaz.com/tupian/renwutupian_187.html)
2025-03-26 17:43:01 [scrapy.extensions.logstats] INFO: Crawled 194 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:43:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_193.html> (referer: https://sc.chinaz.com/tupian/renwutupian_192.html)
2025-03-26 17:43:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_194.html> (referer: https://sc.chinaz.com/tupian/renwutupian_192.html)
2025-03-26 17:43:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_195.html> (referer: https://sc.chinaz.com/tupian/renwutupian_192.html)
2025-03-26 17:43:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_196.html> (referer: https://sc.chinaz.com/tupian/renwutupian_192.html)
2025-03-26 17:43:01 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_197.html> (referer: https://sc.chinaz.com/tupian/renwutupian_192.html)
2025-03-26 17:43:40 [scrapy.extensions.logstats] INFO: Crawled 199 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:44:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_199.html> (referer: https://sc.chinaz.com/tupian/renwutupian_197.html)
2025-03-26 17:44:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_200.html> (referer: https://sc.chinaz.com/tupian/renwutupian_197.html)
2025-03-26 17:44:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_198.html> (referer: https://sc.chinaz.com/tupian/renwutupian_197.html)
2025-03-26 17:44:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_201.html> (referer: https://sc.chinaz.com/tupian/renwutupian_197.html)
2025-03-26 17:44:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_202.html> (referer: https://sc.chinaz.com/tupian/renwutupian_197.html)
2025-03-26 17:45:29 [scrapy.extensions.logstats] INFO: Crawled 204 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:45:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_203.html> (referer: https://sc.chinaz.com/tupian/renwutupian_202.html)
2025-03-26 17:45:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_204.html> (referer: https://sc.chinaz.com/tupian/renwutupian_202.html)
2025-03-26 17:45:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_205.html> (referer: https://sc.chinaz.com/tupian/renwutupian_202.html)
2025-03-26 17:45:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_206.html> (referer: https://sc.chinaz.com/tupian/renwutupian_202.html)
2025-03-26 17:45:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_207.html> (referer: https://sc.chinaz.com/tupian/renwutupian_202.html)
2025-03-26 17:45:40 [scrapy.extensions.logstats] INFO: Crawled 209 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:46:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_206.html> (referer: https://sc.chinaz.com/tupian/renwutupian_202.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 17:46:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_208.html> (referer: https://sc.chinaz.com/tupian/renwutupian_207.html)
2025-03-26 17:46:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_210.html> (referer: https://sc.chinaz.com/tupian/renwutupian_207.html)
2025-03-26 17:46:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_209.html> (referer: https://sc.chinaz.com/tupian/renwutupian_207.html)
2025-03-26 17:46:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_212.html> (referer: https://sc.chinaz.com/tupian/renwutupian_207.html)
2025-03-26 17:46:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_211.html> (referer: https://sc.chinaz.com/tupian/renwutupian_207.html)
2025-03-26 17:46:23 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_208.html> (referer: https://sc.chinaz.com/tupian/renwutupian_207.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 17:47:18 [scrapy.extensions.logstats] INFO: Crawled 214 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:47:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_213.html> (referer: https://sc.chinaz.com/tupian/renwutupian_212.html)
2025-03-26 17:47:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_215.html> (referer: https://sc.chinaz.com/tupian/renwutupian_212.html)
2025-03-26 17:47:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_216.html> (referer: https://sc.chinaz.com/tupian/renwutupian_212.html)
2025-03-26 17:47:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_214.html> (referer: https://sc.chinaz.com/tupian/renwutupian_212.html)
2025-03-26 17:47:18 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_217.html> (referer: https://sc.chinaz.com/tupian/renwutupian_212.html)
2025-03-26 17:47:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_216.html> (referer: https://sc.chinaz.com/tupian/renwutupian_212.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 17:48:25 [scrapy.extensions.logstats] INFO: Crawled 219 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:48:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_221.html> (referer: https://sc.chinaz.com/tupian/renwutupian_217.html)
2025-03-26 17:48:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_222.html> (referer: https://sc.chinaz.com/tupian/renwutupian_217.html)
2025-03-26 17:48:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_218.html> (referer: https://sc.chinaz.com/tupian/renwutupian_217.html)
2025-03-26 17:48:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_219.html> (referer: https://sc.chinaz.com/tupian/renwutupian_217.html)
2025-03-26 17:48:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_220.html> (referer: https://sc.chinaz.com/tupian/renwutupian_217.html)
2025-03-26 17:48:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_222.html> (referer: https://sc.chinaz.com/tupian/renwutupian_217.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 17:49:02 [scrapy.extensions.logstats] INFO: Crawled 224 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:49:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_226.html> (referer: https://sc.chinaz.com/tupian/renwutupian_221.html)
2025-03-26 17:49:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_225.html> (referer: https://sc.chinaz.com/tupian/renwutupian_221.html)
2025-03-26 17:49:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_224.html> (referer: https://sc.chinaz.com/tupian/renwutupian_221.html)
2025-03-26 17:49:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_223.html> (referer: https://sc.chinaz.com/tupian/renwutupian_221.html)
2025-03-26 17:49:37 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_226.html> (referer: https://sc.chinaz.com/tupian/renwutupian_221.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 17:50:16 [scrapy.extensions.logstats] INFO: Crawled 228 pages (at 4 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:50:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_227.html> (referer: https://sc.chinaz.com/tupian/renwutupian_225.html)
2025-03-26 17:50:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_228.html> (referer: https://sc.chinaz.com/tupian/renwutupian_225.html)
2025-03-26 17:50:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_230.html> (referer: https://sc.chinaz.com/tupian/renwutupian_225.html)
2025-03-26 17:50:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_229.html> (referer: https://sc.chinaz.com/tupian/renwutupian_225.html)
2025-03-26 17:50:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_227.html> (referer: https://sc.chinaz.com/tupian/renwutupian_225.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 17:50:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_228.html> (referer: https://sc.chinaz.com/tupian/renwutupian_225.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 17:50:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_230.html> (referer: https://sc.chinaz.com/tupian/renwutupian_225.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 17:50:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_232.html> (referer: https://sc.chinaz.com/tupian/renwutupian_229.html)
2025-03-26 17:50:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_231.html> (referer: https://sc.chinaz.com/tupian/renwutupian_229.html)
2025-03-26 17:50:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_234.html> (referer: https://sc.chinaz.com/tupian/renwutupian_229.html)
2025-03-26 17:50:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_233.html> (referer: https://sc.chinaz.com/tupian/renwutupian_229.html)
2025-03-26 17:50:47 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_231.html> (referer: https://sc.chinaz.com/tupian/renwutupian_229.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 17:51:01 [scrapy.extensions.logstats] INFO: Crawled 236 pages (at 8 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:51:19 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_233.html> (referer: https://sc.chinaz.com/tupian/renwutupian_229.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 17:51:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_235.html> (referer: https://sc.chinaz.com/tupian/renwutupian_234.html)
2025-03-26 17:51:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_236.html> (referer: https://sc.chinaz.com/tupian/renwutupian_234.html)
2025-03-26 17:51:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_237.html> (referer: https://sc.chinaz.com/tupian/renwutupian_234.html)
2025-03-26 17:51:56 [scrapy.extensions.logstats] INFO: Crawled 239 pages (at 3 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:51:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_239.html> (referer: https://sc.chinaz.com/tupian/renwutupian_234.html)
2025-03-26 17:51:56 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_238.html> (referer: https://sc.chinaz.com/tupian/renwutupian_234.html)
2025-03-26 17:52:43 [scrapy.extensions.logstats] INFO: Crawled 241 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:52:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_240.html> (referer: https://sc.chinaz.com/tupian/renwutupian_236.html)
2025-03-26 17:52:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_241.html> (referer: https://sc.chinaz.com/tupian/renwutupian_236.html)
2025-03-26 17:52:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_242.html> (referer: https://sc.chinaz.com/tupian/renwutupian_237.html)
2025-03-26 17:52:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_243.html> (referer: https://sc.chinaz.com/tupian/renwutupian_239.html)
2025-03-26 17:52:43 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_244.html> (referer: https://sc.chinaz.com/tupian/renwutupian_239.html)
2025-03-26 17:52:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_240.html> (referer: https://sc.chinaz.com/tupian/renwutupian_236.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 17:53:51 [scrapy.extensions.logstats] INFO: Crawled 246 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:53:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_248.html> (referer: https://sc.chinaz.com/tupian/renwutupian_244.html)
2025-03-26 17:53:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_246.html> (referer: https://sc.chinaz.com/tupian/renwutupian_244.html)
2025-03-26 17:53:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_245.html> (referer: https://sc.chinaz.com/tupian/renwutupian_244.html)
2025-03-26 17:53:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_247.html> (referer: https://sc.chinaz.com/tupian/renwutupian_244.html)
2025-03-26 17:53:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_249.html> (referer: https://sc.chinaz.com/tupian/renwutupian_244.html)
2025-03-26 17:54:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_249.html> (referer: https://sc.chinaz.com/tupian/renwutupian_244.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 17:54:54 [scrapy.extensions.logstats] INFO: Crawled 251 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_250.html> (referer: https://sc.chinaz.com/tupian/renwutupian_248.html)
2025-03-26 17:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_252.html> (referer: https://sc.chinaz.com/tupian/renwutupian_248.html)
2025-03-26 17:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_251.html> (referer: https://sc.chinaz.com/tupian/renwutupian_248.html)
2025-03-26 17:54:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_253.html> (referer: https://sc.chinaz.com/tupian/renwutupian_248.html)
2025-03-26 17:55:40 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_251.html> (referer: https://sc.chinaz.com/tupian/renwutupian_248.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 17:55:53 [scrapy.extensions.logstats] INFO: Crawled 255 pages (at 4 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:55:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_254.html> (referer: https://sc.chinaz.com/tupian/renwutupian_253.html)
2025-03-26 17:55:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_255.html> (referer: https://sc.chinaz.com/tupian/renwutupian_253.html)
2025-03-26 17:55:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_256.html> (referer: https://sc.chinaz.com/tupian/renwutupian_253.html)
2025-03-26 17:55:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_257.html> (referer: https://sc.chinaz.com/tupian/renwutupian_253.html)
2025-03-26 17:55:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_258.html> (referer: https://sc.chinaz.com/tupian/renwutupian_253.html)
2025-03-26 17:56:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_256.html> (referer: https://sc.chinaz.com/tupian/renwutupian_253.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 17:56:32 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_258.html> (referer: https://sc.chinaz.com/tupian/renwutupian_253.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 17:56:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_261.html> (referer: https://sc.chinaz.com/tupian/renwutupian_257.html)
2025-03-26 17:56:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_259.html> (referer: https://sc.chinaz.com/tupian/renwutupian_257.html)
2025-03-26 17:56:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_260.html> (referer: https://sc.chinaz.com/tupian/renwutupian_257.html)
2025-03-26 17:56:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_262.html> (referer: https://sc.chinaz.com/tupian/renwutupian_257.html)
2025-03-26 17:56:59 [scrapy.extensions.logstats] INFO: Crawled 264 pages (at 9 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:57:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_263.html> (referer: https://sc.chinaz.com/tupian/renwutupian_261.html)
2025-03-26 17:57:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_266.html> (referer: https://sc.chinaz.com/tupian/renwutupian_261.html)
2025-03-26 17:57:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_265.html> (referer: https://sc.chinaz.com/tupian/renwutupian_261.html)
2025-03-26 17:57:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_264.html> (referer: https://sc.chinaz.com/tupian/renwutupian_261.html)
2025-03-26 17:57:31 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_263.html> (referer: https://sc.chinaz.com/tupian/renwutupian_261.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 17:57:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_267.html> (referer: https://sc.chinaz.com/tupian/renwutupian_262.html)
2025-03-26 17:58:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_265.html> (referer: https://sc.chinaz.com/tupian/renwutupian_261.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 17:58:06 [scrapy.extensions.logstats] INFO: Crawled 269 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:58:06 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_267.html> (referer: https://sc.chinaz.com/tupian/renwutupian_262.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 17:58:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_268.html> (referer: https://sc.chinaz.com/tupian/renwutupian_266.html)
2025-03-26 17:58:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_270.html> (referer: https://sc.chinaz.com/tupian/renwutupian_266.html)
2025-03-26 17:58:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_271.html> (referer: https://sc.chinaz.com/tupian/renwutupian_266.html)
2025-03-26 17:58:07 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_269.html> (referer: https://sc.chinaz.com/tupian/renwutupian_266.html)
2025-03-26 17:59:05 [scrapy.extensions.logstats] INFO: Crawled 273 pages (at 4 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 17:59:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_272.html> (referer: https://sc.chinaz.com/tupian/renwutupian_271.html)
2025-03-26 17:59:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_273.html> (referer: https://sc.chinaz.com/tupian/renwutupian_271.html)
2025-03-26 17:59:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_274.html> (referer: https://sc.chinaz.com/tupian/renwutupian_271.html)
2025-03-26 17:59:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_275.html> (referer: https://sc.chinaz.com/tupian/renwutupian_271.html)
2025-03-26 17:59:36 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_273.html> (referer: https://sc.chinaz.com/tupian/renwutupian_271.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 18:00:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_276.html> (referer: https://sc.chinaz.com/tupian/renwutupian_271.html)
2025-03-26 18:00:14 [scrapy.extensions.logstats] INFO: Crawled 278 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 18:00:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_279.html> (referer: https://sc.chinaz.com/tupian/renwutupian_275.html)
2025-03-26 18:00:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_278.html> (referer: https://sc.chinaz.com/tupian/renwutupian_275.html)
2025-03-26 18:00:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_277.html> (referer: https://sc.chinaz.com/tupian/renwutupian_275.html)
2025-03-26 18:00:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_280.html> (referer: https://sc.chinaz.com/tupian/renwutupian_275.html)
2025-03-26 18:01:17 [scrapy.extensions.logstats] INFO: Crawled 282 pages (at 4 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 18:01:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_281.html> (referer: https://sc.chinaz.com/tupian/renwutupian_276.html)
2025-03-26 18:01:30 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_281.html> (referer: https://sc.chinaz.com/tupian/renwutupian_276.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 18:01:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_283.html> (referer: https://sc.chinaz.com/tupian/renwutupian_279.html)
2025-03-26 18:01:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_284.html> (referer: https://sc.chinaz.com/tupian/renwutupian_279.html)
2025-03-26 18:01:30 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_282.html> (referer: https://sc.chinaz.com/tupian/renwutupian_279.html)
2025-03-26 18:01:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_284.html> (referer: https://sc.chinaz.com/tupian/renwutupian_279.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 18:01:44 [scrapy.extensions.logstats] INFO: Crawled 286 pages (at 4 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 18:01:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_285.html> (referer: https://sc.chinaz.com/tupian/renwutupian_280.html)
2025-03-26 18:02:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_286.html> (referer: https://sc.chinaz.com/tupian/renwutupian_283.html)
2025-03-26 18:02:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_288.html> (referer: https://sc.chinaz.com/tupian/renwutupian_283.html)
2025-03-26 18:02:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_287.html> (referer: https://sc.chinaz.com/tupian/renwutupian_283.html)
2025-03-26 18:02:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_289.html> (referer: https://sc.chinaz.com/tupian/renwutupian_285.html)
2025-03-26 18:03:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_290.html> (referer: https://sc.chinaz.com/tupian/renwutupian_285.html)
2025-03-26 18:03:03 [scrapy.extensions.logstats] INFO: Crawled 292 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 18:03:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_289.html> (referer: https://sc.chinaz.com/tupian/renwutupian_285.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 18:03:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_291.html> (referer: https://sc.chinaz.com/tupian/renwutupian_288.html)
2025-03-26 18:03:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_295.html> (referer: https://sc.chinaz.com/tupian/renwutupian_290.html)
2025-03-26 18:03:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_293.html> (referer: https://sc.chinaz.com/tupian/renwutupian_288.html)
2025-03-26 18:03:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_292.html> (referer: https://sc.chinaz.com/tupian/renwutupian_288.html)
2025-03-26 18:03:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_294.html> (referer: https://sc.chinaz.com/tupian/renwutupian_290.html)
2025-03-26 18:03:22 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_291.html> (referer: https://sc.chinaz.com/tupian/renwutupian_288.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 18:03:34 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_293.html> (referer: https://sc.chinaz.com/tupian/renwutupian_288.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 18:03:51 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_294.html> (referer: https://sc.chinaz.com/tupian/renwutupian_290.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 18:03:51 [scrapy.extensions.logstats] INFO: Crawled 297 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 18:03:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_296.html> (referer: https://sc.chinaz.com/tupian/renwutupian_295.html)
2025-03-26 18:03:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_297.html> (referer: https://sc.chinaz.com/tupian/renwutupian_295.html)
2025-03-26 18:03:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_298.html> (referer: https://sc.chinaz.com/tupian/renwutupian_295.html)
2025-03-26 18:03:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_300.html> (referer: https://sc.chinaz.com/tupian/renwutupian_295.html)
2025-03-26 18:03:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_299.html> (referer: https://sc.chinaz.com/tupian/renwutupian_295.html)
2025-03-26 18:04:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_298.html> (referer: https://sc.chinaz.com/tupian/renwutupian_295.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 18:04:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_300.html> (referer: https://sc.chinaz.com/tupian/renwutupian_295.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 18:04:26 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_299.html> (referer: https://sc.chinaz.com/tupian/renwutupian_295.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 18:04:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_302.html> (referer: https://sc.chinaz.com/tupian/renwutupian_297.html)
2025-03-26 18:04:26 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_301.html> (referer: https://sc.chinaz.com/tupian/renwutupian_297.html)
2025-03-26 18:04:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_302.html> (referer: https://sc.chinaz.com/tupian/renwutupian_297.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 18:04:40 [scrapy.extensions.logstats] INFO: Crawled 304 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 18:04:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_305.html> (referer: https://sc.chinaz.com/tupian/renwutupian_301.html)
2025-03-26 18:04:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_304.html> (referer: https://sc.chinaz.com/tupian/renwutupian_301.html)
2025-03-26 18:04:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_306.html> (referer: https://sc.chinaz.com/tupian/renwutupian_301.html)
2025-03-26 18:04:40 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_303.html> (referer: https://sc.chinaz.com/tupian/renwutupian_301.html)
2025-03-26 18:05:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_308.html> (referer: https://sc.chinaz.com/tupian/renwutupian_305.html)
2025-03-26 18:05:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_307.html> (referer: https://sc.chinaz.com/tupian/renwutupian_305.html)
2025-03-26 18:05:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_310.html> (referer: https://sc.chinaz.com/tupian/renwutupian_305.html)
2025-03-26 18:05:39 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_309.html> (referer: https://sc.chinaz.com/tupian/renwutupian_305.html)
2025-03-26 18:05:39 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_308.html> (referer: https://sc.chinaz.com/tupian/renwutupian_305.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 18:05:55 [scrapy.extensions.logstats] INFO: Crawled 312 pages (at 8 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 18:05:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_310.html> (referer: https://sc.chinaz.com/tupian/renwutupian_305.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 18:05:57 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_309.html> (referer: https://sc.chinaz.com/tupian/renwutupian_305.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 18:05:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_311.html> (referer: https://sc.chinaz.com/tupian/renwutupian_306.html)
2025-03-26 18:05:58 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/renwutupian_311.html> (referer: https://sc.chinaz.com/tupian/renwutupian_306.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./renwu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-26 18:05:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_312.html> (referer: https://sc.chinaz.com/tupian/renwutupian_307.html)
2025-03-26 18:06:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_314.html> (referer: https://sc.chinaz.com/tupian/renwutupian_312.html)
2025-03-26 18:06:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_315.html> (referer: https://sc.chinaz.com/tupian/renwutupian_312.html)
2025-03-26 18:06:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_317.html> (referer: https://sc.chinaz.com/tupian/renwutupian_312.html)
2025-03-26 18:06:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_316.html> (referer: https://sc.chinaz.com/tupian/renwutupian_312.html)
2025-03-26 18:06:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_313.html> (referer: https://sc.chinaz.com/tupian/renwutupian_312.html)
2025-03-26 18:07:32 [scrapy.extensions.logstats] INFO: Crawled 319 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 18:07:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_321.html> (referer: https://sc.chinaz.com/tupian/renwutupian_317.html)
2025-03-26 18:07:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_319.html> (referer: https://sc.chinaz.com/tupian/renwutupian_317.html)
2025-03-26 18:07:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_318.html> (referer: https://sc.chinaz.com/tupian/renwutupian_317.html)
2025-03-26 18:07:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_322.html> (referer: https://sc.chinaz.com/tupian/renwutupian_317.html)
2025-03-26 18:07:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_320.html> (referer: https://sc.chinaz.com/tupian/renwutupian_317.html)
2025-03-26 18:07:59 [scrapy.extensions.logstats] INFO: Crawled 324 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 18:08:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_327.html> (referer: https://sc.chinaz.com/tupian/renwutupian_322.html)
2025-03-26 18:08:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_326.html> (referer: https://sc.chinaz.com/tupian/renwutupian_321.html)
2025-03-26 18:08:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_323.html> (referer: https://sc.chinaz.com/tupian/renwutupian_321.html)
2025-03-26 18:08:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_325.html> (referer: https://sc.chinaz.com/tupian/renwutupian_321.html)
2025-03-26 18:08:38 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_324.html> (referer: https://sc.chinaz.com/tupian/renwutupian_321.html)
2025-03-26 18:09:18 [scrapy.extensions.logstats] INFO: Crawled 329 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 18:09:45 [scrapy.extensions.logstats] INFO: Crawled 329 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 18:09:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_328.html> (referer: https://sc.chinaz.com/tupian/renwutupian_327.html)
2025-03-26 18:09:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_329.html> (referer: https://sc.chinaz.com/tupian/renwutupian_327.html)
2025-03-26 18:09:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_332.html> (referer: https://sc.chinaz.com/tupian/renwutupian_327.html)
2025-03-26 18:09:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_330.html> (referer: https://sc.chinaz.com/tupian/renwutupian_327.html)
2025-03-26 18:09:45 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_331.html> (referer: https://sc.chinaz.com/tupian/renwutupian_327.html)
2025-03-26 18:10:59 [scrapy.extensions.logstats] INFO: Crawled 334 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 18:10:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_333.html> (referer: https://sc.chinaz.com/tupian/renwutupian_332.html)
2025-03-26 18:10:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_334.html> (referer: https://sc.chinaz.com/tupian/renwutupian_332.html)
2025-03-26 18:10:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_335.html> (referer: https://sc.chinaz.com/tupian/renwutupian_332.html)
2025-03-26 18:10:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_336.html> (referer: https://sc.chinaz.com/tupian/renwutupian_332.html)
2025-03-26 18:10:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_337.html> (referer: https://sc.chinaz.com/tupian/renwutupian_332.html)
2025-03-26 18:12:09 [scrapy.extensions.logstats] INFO: Crawled 339 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 18:12:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_338.html> (referer: https://sc.chinaz.com/tupian/renwutupian_337.html)
2025-03-26 18:12:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_340.html> (referer: https://sc.chinaz.com/tupian/renwutupian_337.html)
2025-03-26 18:12:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_339.html> (referer: https://sc.chinaz.com/tupian/renwutupian_337.html)
2025-03-26 18:12:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_342.html> (referer: https://sc.chinaz.com/tupian/renwutupian_337.html)
2025-03-26 18:12:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_341.html> (referer: https://sc.chinaz.com/tupian/renwutupian_337.html)
2025-03-26 18:13:25 [scrapy.extensions.logstats] INFO: Crawled 344 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 18:13:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_345.html> (referer: https://sc.chinaz.com/tupian/renwutupian_342.html)
2025-03-26 18:13:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_347.html> (referer: https://sc.chinaz.com/tupian/renwutupian_342.html)
2025-03-26 18:13:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_343.html> (referer: https://sc.chinaz.com/tupian/renwutupian_342.html)
2025-03-26 18:13:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_346.html> (referer: https://sc.chinaz.com/tupian/renwutupian_342.html)
2025-03-26 18:13:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_344.html> (referer: https://sc.chinaz.com/tupian/renwutupian_342.html)
2025-03-26 18:14:16 [scrapy.extensions.logstats] INFO: Crawled 349 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 18:14:49 [scrapy.extensions.logstats] INFO: Crawled 349 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 18:14:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_348.html> (referer: https://sc.chinaz.com/tupian/renwutupian_347.html)
2025-03-26 18:15:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_351.html> (referer: https://sc.chinaz.com/tupian/renwutupian_347.html)
2025-03-26 18:15:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_349.html> (referer: https://sc.chinaz.com/tupian/renwutupian_347.html)
2025-03-26 18:15:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_350.html> (referer: https://sc.chinaz.com/tupian/renwutupian_347.html)
2025-03-26 18:15:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_353.html> (referer: https://sc.chinaz.com/tupian/renwutupian_348.html)
2025-03-26 18:15:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_352.html> (referer: https://sc.chinaz.com/tupian/renwutupian_347.html)
2025-03-26 18:15:55 [scrapy.extensions.logstats] INFO: Crawled 355 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 18:16:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_355.html> (referer: https://sc.chinaz.com/tupian/renwutupian_351.html)
2025-03-26 18:16:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_354.html> (referer: https://sc.chinaz.com/tupian/renwutupian_351.html)
2025-03-26 18:16:59 [scrapy.extensions.logstats] INFO: Crawled 357 pages (at 2 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 18:16:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_356.html> (referer: https://sc.chinaz.com/tupian/renwutupian_351.html)
2025-03-26 18:17:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_358.html> (referer: https://sc.chinaz.com/tupian/renwutupian_353.html)
2025-03-26 18:17:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_359.html> (referer: https://sc.chinaz.com/tupian/renwutupian_355.html)
2025-03-26 18:17:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_360.html> (referer: https://sc.chinaz.com/tupian/renwutupian_355.html)
2025-03-26 18:17:14 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_357.html> (referer: https://sc.chinaz.com/tupian/renwutupian_353.html)
2025-03-26 18:17:32 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-03-26 18:17:48 [scrapy.extensions.logstats] INFO: Crawled 362 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-26 18:17:48 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/renwutupian_361.html> (referer: https://sc.chinaz.com/tupian/renwutupian_356.html)
2025-03-26 18:17:48 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-03-28 09:35:43 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: yolov8_)
2025-03-28 09:35:43 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.11.7, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Windows-11-10.0.26100-SP0
2025-03-28 09:35:43 [scrapy.addons] INFO: Enabled addons:
[]
2025-03-28 09:35:43 [asyncio] DEBUG: Using selector: SelectSelector
2025-03-28 09:35:43 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2025-03-28 09:35:43 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
2025-03-28 09:35:43 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2025-03-28 09:35:43 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
2025-03-28 09:35:43 [scrapy.extensions.telnet] INFO: Telnet Password: cca48c2fcb6b43b7
2025-03-28 09:35:43 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2025-03-28 09:35:43 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'yolov8_',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'tupian_.log',
 'NEWSPIDER_MODULE': 'yolov8_.spiders',
 'SPIDER_MODULES': ['yolov8_.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2025-03-28 09:35:43 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-03-28 09:35:43 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-03-28 09:35:43 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-03-28 09:35:43 [scrapy.core.engine] INFO: Spider opened
2025-03-28 09:35:43 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-28 09:35:43 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-03-28 09:35:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/minzuyishutupian.html> (referer: None)
2025-03-28 09:35:44 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/minzuyishutupian_2.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian.html)
2025-03-28 09:36:00 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://sc.chinaz.com/tupian/minzuyishutupian_3.html> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2025-03-28 09:36:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/minzuyishutupian_6.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian.html)
2025-03-28 09:36:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/minzuyishutupian_5.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian.html)
2025-03-28 09:36:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/minzuyishutupian_68.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian.html)
2025-03-28 09:36:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/minzuyishutupian_3.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian.html)
2025-03-28 09:36:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/minzuyishutupian_4.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian.html)
2025-03-28 09:36:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/minzuyishutupian_7.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian_2.html)
2025-03-28 09:36:36 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/minzuyishutupian_68.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./mingzuyishu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-28 09:37:11 [scrapy.extensions.logstats] INFO: Crawled 8 pages (at 8 pages/min), scraped 0 items (at 0 items/min)
2025-03-28 09:37:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/minzuyishutupian_8.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian_6.html)
2025-03-28 09:37:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/minzuyishutupian_11.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian_6.html)
2025-03-28 09:37:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/minzuyishutupian_9.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian_6.html)
2025-03-28 09:37:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/minzuyishutupian_10.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian_6.html)
2025-03-28 09:37:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/minzuyishutupian_12.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian_7.html)
2025-03-28 09:38:14 [scrapy.extensions.logstats] INFO: Crawled 13 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-28 09:38:46 [scrapy.extensions.logstats] INFO: Crawled 13 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-28 09:38:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/minzuyishutupian_13.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian_11.html)
2025-03-28 09:38:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/minzuyishutupian_14.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian_11.html)
2025-03-28 09:38:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/minzuyishutupian_15.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian_11.html)
2025-03-28 09:38:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/minzuyishutupian_16.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian_11.html)
2025-03-28 09:38:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/minzuyishutupian_17.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian_12.html)
2025-03-28 09:39:46 [scrapy.extensions.logstats] INFO: Crawled 18 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-28 09:39:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/minzuyishutupian_18.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian_17.html)
2025-03-28 09:39:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/minzuyishutupian_19.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian_17.html)
2025-03-28 09:39:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/minzuyishutupian_22.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian_17.html)
2025-03-28 09:39:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/minzuyishutupian_20.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian_17.html)
2025-03-28 09:39:46 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/minzuyishutupian_21.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian_17.html)
2025-03-28 09:40:53 [scrapy.extensions.logstats] INFO: Crawled 23 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-28 09:40:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/minzuyishutupian_23.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian_22.html)
2025-03-28 09:40:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/minzuyishutupian_24.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian_22.html)
2025-03-28 09:40:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/minzuyishutupian_25.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian_22.html)
2025-03-28 09:40:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/minzuyishutupian_27.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian_22.html)
2025-03-28 09:40:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/minzuyishutupian_26.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian_22.html)
2025-03-28 09:40:53 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/minzuyishutupian_61.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian_22.html)
2025-03-28 09:41:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/minzuyishutupian_25.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian_22.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./mingzuyishu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-28 09:41:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/minzuyishutupian_27.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian_22.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./mingzuyishu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-28 09:41:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/minzuyishutupian_26.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian_22.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./mingzuyishu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-28 09:41:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/minzuyishutupian_61.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian_22.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./mingzuyishu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-28 09:41:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/minzuyishutupian_28.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian_24.html)
2025-03-28 09:41:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/minzuyishutupian_29.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian_24.html)
2025-03-28 09:41:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/minzuyishutupian_28.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian_24.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./mingzuyishu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-28 09:41:28 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/minzuyishutupian_29.html> (referer: https://sc.chinaz.com/tupian/minzuyishutupian_24.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./mingzuyishu/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-28 09:41:28 [scrapy.core.engine] INFO: Closing spider (finished)
2025-03-28 09:41:28 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 9790,
 'downloader/request_count': 31,
 'downloader/request_method_count/GET': 31,
 'downloader/response_bytes': 188859,
 'downloader/response_count': 31,
 'downloader/response_status_count/200': 31,
 'dupefilter/filtered': 214,
 'elapsed_time_seconds': 344.374205,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 3, 28, 1, 41, 28, 287788, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 1058375,
 'httpcompression/response_count': 31,
 'items_per_minute': None,
 'log_count/DEBUG': 37,
 'log_count/ERROR': 7,
 'log_count/INFO': 15,
 'request_depth_max': 7,
 'response_received_count': 31,
 'responses_per_minute': None,
 'scheduler/dequeued': 31,
 'scheduler/dequeued/memory': 31,
 'scheduler/enqueued': 31,
 'scheduler/enqueued/memory': 31,
 'spider_exceptions/HTTPError': 7,
 'start_time': datetime.datetime(2025, 3, 28, 1, 35, 43, 913583, tzinfo=datetime.timezone.utc)}
2025-03-28 09:41:28 [scrapy.core.engine] INFO: Spider closed (finished)
2025-03-28 10:32:56 [scrapy.utils.log] INFO: Scrapy 2.12.0 started (bot: yolov8_)
2025-03-28 10:32:56 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.11.7, cssselect 1.2.0, parsel 1.10.0, w3lib 2.3.1, Twisted 24.11.0, Python 3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)], pyOpenSSL 25.0.0 (OpenSSL 3.4.1 11 Feb 2025), cryptography 44.0.1, Platform Windows-11-10.0.26100-SP0
2025-03-28 10:32:56 [scrapy.addons] INFO: Enabled addons:
[]
2025-03-28 10:32:56 [asyncio] DEBUG: Using selector: SelectSelector
2025-03-28 10:32:56 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2025-03-28 10:32:56 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
2025-03-28 10:32:56 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor
2025-03-28 10:32:56 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.windows_events._WindowsSelectorEventLoop
2025-03-28 10:32:56 [scrapy.extensions.telnet] INFO: Telnet Password: ba53f7bab645560c
2025-03-28 10:32:56 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2025-03-28 10:32:56 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'yolov8_',
 'FEED_EXPORT_ENCODING': 'utf-8',
 'LOG_FILE': 'tupian_.log',
 'NEWSPIDER_MODULE': 'yolov8_.spiders',
 'SPIDER_MODULES': ['yolov8_.spiders'],
 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}
2025-03-28 10:32:57 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-03-28 10:32:57 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-03-28 10:32:57 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2025-03-28 10:32:57 [scrapy.core.engine] INFO: Spider opened
2025-03-28 10:32:57 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-28 10:32:57 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-03-28 10:32:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian.html> (referer: None)
2025-03-28 10:32:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_2.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian.html)
2025-03-28 10:32:57 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_5.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian.html)
2025-03-28 10:33:19 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://sc.chinaz.com/tupian/shuiziyuantupian_3.html> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
2025-03-28 10:33:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_53.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian.html)
2025-03-28 10:33:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_6.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian.html)
2025-03-28 10:33:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_3.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian.html)
2025-03-28 10:33:19 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_4.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian.html)
2025-03-28 10:33:59 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/shuiziyuantupian_53.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shuiziyuan/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-28 10:34:36 [scrapy.extensions.logstats] INFO: Crawled 7 pages (at 7 pages/min), scraped 0 items (at 0 items/min)
2025-03-28 10:34:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_10.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_5.html)
2025-03-28 10:34:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_7.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_2.html)
2025-03-28 10:34:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_8.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_5.html)
2025-03-28 10:34:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_9.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_5.html)
2025-03-28 10:34:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_11.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_6.html)
2025-03-28 10:35:29 [scrapy.extensions.logstats] INFO: Crawled 12 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-28 10:35:59 [scrapy.extensions.logstats] INFO: Crawled 12 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-03-28 10:36:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_12.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_10.html)
2025-03-28 10:36:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_14.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_10.html)
2025-03-28 10:36:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_15.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_10.html)
2025-03-28 10:36:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_13.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_10.html)
2025-03-28 10:36:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_16.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_11.html)
2025-03-28 10:37:05 [scrapy.extensions.logstats] INFO: Crawled 17 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-28 10:37:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_17.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_16.html)
2025-03-28 10:37:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_20.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_16.html)
2025-03-28 10:37:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_18.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_16.html)
2025-03-28 10:37:06 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_21.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_16.html)
2025-03-28 10:37:22 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_19.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_16.html)
2025-03-28 10:38:07 [scrapy.extensions.logstats] INFO: Crawled 22 pages (at 5 pages/min), scraped 0 items (at 0 items/min)
2025-03-28 10:38:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_23.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_21.html)
2025-03-28 10:38:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_24.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_21.html)
2025-03-28 10:38:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_25.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_21.html)
2025-03-28 10:38:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_22.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_21.html)
2025-03-28 10:38:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_26.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_21.html)
2025-03-28 10:38:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_45.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_21.html)
2025-03-28 10:39:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/shuiziyuantupian_45.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_21.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shuiziyuan/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-28 10:39:20 [scrapy.extensions.logstats] INFO: Crawled 28 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2025-03-28 10:39:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_28.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_26.html)
2025-03-28 10:39:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_29.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_26.html)
2025-03-28 10:39:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_27.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_26.html)
2025-03-28 10:39:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_30.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_26.html)
2025-03-28 10:39:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_31.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_26.html)
2025-03-28 10:39:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_46.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_26.html)
2025-03-28 10:39:27 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/shuiziyuantupian_28.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_26.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shuiziyuan/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-28 10:40:03 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/shuiziyuantupian_46.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_26.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shuiziyuan/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-28 10:40:03 [scrapy.extensions.logstats] INFO: Crawled 34 pages (at 6 pages/min), scraped 0 items (at 0 items/min)
2025-03-28 10:40:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_32.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_31.html)
2025-03-28 10:40:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_34.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_31.html)
2025-03-28 10:40:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_33.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_31.html)
2025-03-28 10:40:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_49.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_29.html)
2025-03-28 10:40:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_35.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_31.html)
2025-03-28 10:40:03 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_36.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_31.html)
2025-03-28 10:40:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_50.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_30.html)
2025-03-28 10:40:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_47.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_27.html)
2025-03-28 10:40:44 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/shuiziyuantupian_49.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_29.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shuiziyuan/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-28 10:41:09 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_51.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_31.html)
2025-03-28 10:41:09 [scrapy.extensions.logstats] INFO: Crawled 43 pages (at 9 pages/min), scraped 0 items (at 0 items/min)
2025-03-28 10:41:09 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/shuiziyuantupian_50.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_30.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shuiziyuan/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-28 10:41:09 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/shuiziyuantupian_47.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_27.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shuiziyuan/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-28 10:41:09 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/shuiziyuantupian_51.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_31.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shuiziyuan/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-28 10:41:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_37.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_36.html)
2025-03-28 10:41:10 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_39.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_36.html)
2025-03-28 10:41:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/shuiziyuantupian_37.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_36.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shuiziyuan/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-28 10:41:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_40.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_36.html)
2025-03-28 10:41:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_41.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_36.html)
2025-03-28 10:41:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_52.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_32.html)
2025-03-28 10:41:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://sc.chinaz.com/tupian/shuiziyuantupian_38.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_36.html)
2025-03-28 10:41:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/shuiziyuantupian_39.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_36.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shuiziyuan/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-28 10:41:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/shuiziyuantupian_40.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_36.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shuiziyuan/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-28 10:41:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/shuiziyuantupian_41.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_36.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shuiziyuan/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-28 10:41:20 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/shuiziyuantupian_52.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_32.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shuiziyuan/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 403: Forbidden
2025-03-28 10:41:21 [scrapy.core.scraper] ERROR: Spider error processing <GET https://sc.chinaz.com/tupian/shuiziyuantupian_38.html> (referer: https://sc.chinaz.com/tupian/shuiziyuantupian_36.html)
Traceback (most recent call last):
  File "D:\pt2\Lib\site-packages\scrapy\utils\defer.py", line 346, in aiter_errback
    yield await it.__anext__()
          ^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 394, in __anext__
    return await self.data.__anext__()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\site-packages\scrapy\utils\python.py", line 375, in _async_chain
    async for o in as_async_generator(it):
  File "D:\pt2\Lib\site-packages\scrapy\utils\asyncgen.py", line 21, in as_async_generator
    async for r in it:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\referer.py", line 384, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\urllength.py", line 62, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spidermiddlewares\depth.py", line 60, in process_spider_output_async
    async for r in result:
  File "D:\pt2\Lib\site-packages\scrapy\core\spidermw.py", line 121, in process_async
    async for r in iterable:
  File "D:\pt2\Lib\site-packages\scrapy\spiders\crawl.py", line 161, in _parse_response
    cb_res = callback(response, **cb_kwargs) or ()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "X:\桌面\Scrapy\yolov8_\yolov8_\spiders\tupian_.py", line 27, in parse_item
    urllib.request.urlretrieve(url=img_url, filename='./shuiziyuan/' + img_name + '.jpg')
  File "D:\pt2\Lib\urllib\request.py", line 240, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
                            ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 215, in urlopen
    return opener.open(url, data, timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 521, in open
    response = meth(req, response)
               ^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 630, in http_response
    response = self.parent.error(
               ^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 559, in error
    return self._call_chain(*args)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 492, in _call_chain
    result = func(*args)
             ^^^^^^^^^^^
  File "D:\pt2\Lib\urllib\request.py", line 639, in http_error_default
    raise HTTPError(req.full_url, code, msg, hdrs, fp)
urllib.error.HTTPError: HTTP Error 404: Not Found
2025-03-28 10:41:21 [scrapy.core.engine] INFO: Closing spider (finished)
2025-03-28 10:41:21 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 15533,
 'downloader/request_count': 49,
 'downloader/request_method_count/GET': 49,
 'downloader/response_bytes': 284459,
 'downloader/response_count': 49,
 'downloader/response_status_count/200': 49,
 'dupefilter/filtered': 317,
 'elapsed_time_seconds': 504.036217,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2025, 3, 28, 2, 41, 21, 296470, tzinfo=datetime.timezone.utc),
 'httpcompression/response_bytes': 1636727,
 'httpcompression/response_count': 49,
 'items_per_minute': None,
 'log_count/DEBUG': 55,
 'log_count/ERROR': 14,
 'log_count/INFO': 18,
 'request_depth_max': 8,
 'response_received_count': 49,
 'responses_per_minute': None,
 'scheduler/dequeued': 49,
 'scheduler/dequeued/memory': 49,
 'scheduler/enqueued': 49,
 'scheduler/enqueued/memory': 49,
 'spider_exceptions/HTTPError': 14,
 'start_time': datetime.datetime(2025, 3, 28, 2, 32, 57, 260253, tzinfo=datetime.timezone.utc)}
2025-03-28 10:41:21 [scrapy.core.engine] INFO: Spider closed (finished)
